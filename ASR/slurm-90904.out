 
---------- Step 0: Installing libraries  --------------
 
Collecting numpy==1.23.1
  Using cached numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)
Using cached numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)
Installing collected packages: numpy
Successfully installed numpy-1.23.1
---------- Step 1: Running model ----------------------
/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Some weights of AdaWavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.layers.4.adapter_layer_attn.layernorm.weight', 'encoder.layers.11.adapter_layer_ff.layernorm.weight', 'encoder.layers.3.adapter_layer_attn.layernorm.bias', 'encoder.layers.4.adapter_layer_ff.linear_down.bias', 'encoder.layers.10.adapter_layer_ff.layernorm.weight', 'encoder.layers.6.adapter_layer_ff.linear_up.weight', 'encoder.layers.5.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_up.bias', 'encoder.layers.2.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_ff.linear_up.bias', 'encoder.layers.0.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.layernorm.bias', 'encoder.layers.1.adapter_layer_attn.layernorm.weight', 'encoder.layers.8.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_ff.linear_down.bias', 'encoder.layers.9.adapter_layer_ff.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.layernorm.bias', 'encoder.layers.4.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_attn.layernorm.weight', 'encoder.layers.10.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_attn.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_down.bias', 'encoder.layers.11.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_attn.layernorm.weight', 'encoder.layers.7.adapter_layer_ff.linear_up.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.bias', 'encoder.layers.9.adapter_layer_ff.linear_down.bias', 'encoder.layers.9.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_ff.linear_down.weight', 'encoder.layers.1.adapter_layer_attn.linear_down.weight', 'encoder.layers.2.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.linear_up.weight', 'encoder.layers.0.adapter_layer_ff.linear_up.weight', 'encoder.layers.6.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_attn.linear_down.weight', 'encoder.layers.2.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_attn.linear_up.bias', 'encoder.layers.11.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_ff.linear_down.bias', 'encoder.layers.5.adapter_layer_attn.layernorm.bias', 'encoder.layers.5.adapter_layer_ff.linear_up.bias', 'encoder.layers.5.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_attn.linear_up.weight', 'encoder.layers.0.adapter_layer_attn.layernorm.weight', 'encoder.layers.9.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.layernorm.weight', 'encoder.layers.10.adapter_layer_attn.layernorm.bias', 'encoder.layers.1.adapter_layer_attn.linear_up.bias', 'encoder.layers.2.adapter_layer_ff.linear_down.weight', 'encoder.layers.8.adapter_layer_attn.linear_up.bias', 'encoder.layers.1.adapter_layer_attn.linear_up.weight', 'encoder.layers.10.adapter_layer_attn.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.linear_up.weight', 'encoder.layers.11.adapter_layer_attn.linear_down.bias', 'encoder.layers.0.adapter_layer_attn.linear_up.weight', 'encoder.layers.7.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_attn.linear_down.bias', 'encoder.layers.9.adapter_layer_attn.linear_up.bias', 'encoder.layers.6.adapter_layer_ff.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.weight', 'lm_head.bias', 'encoder.layers.6.adapter_layer_ff.linear_down.bias', 'encoder.layers.6.adapter_layer_ff.layernorm.bias', 'encoder.layers.1.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.linear_down.weight', 'encoder.layers.3.adapter_layer_ff.linear_up.bias', 'encoder.layers.5.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.linear_down.weight', 'encoder.layers.11.adapter_layer_ff.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.bias', 'lm_head.weight', 'encoder.layers.7.adapter_layer_attn.linear_up.weight', 'encoder.layers.8.adapter_layer_ff.layernorm.bias', 'encoder.layers.9.adapter_layer_attn.linear_down.bias', 'encoder.layers.6.adapter_layer_ff.linear_up.bias', 'encoder.layers.11.adapter_layer_ff.linear_down.weight', 'encoder.layers.9.adapter_layer_attn.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.layernorm.weight', 'encoder.layers.7.adapter_layer_ff.linear_up.weight', 'encoder.layers.4.adapter_layer_ff.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.linear_down.weight', 'encoder.layers.2.adapter_layer_attn.linear_down.weight', 'encoder.layers.7.adapter_layer_attn.linear_down.weight', 'encoder.layers.0.adapter_layer_attn.linear_down.weight', 'encoder.layers.4.adapter_layer_attn.linear_up.weight', 'encoder.layers.6.adapter_layer_ff.linear_down.weight', 'encoder.layers.3.adapter_layer_ff.layernorm.weight', 'encoder.layers.8.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_attn.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_down.bias', 'encoder.layers.9.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.layernorm.weight', 'encoder.layers.2.adapter_layer_ff.linear_up.bias', 'encoder.layers.4.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_ff.linear_down.bias', 'encoder.layers.7.adapter_layer_ff.linear_down.bias', 'encoder.layers.1.adapter_layer_ff.linear_up.bias', 'encoder.layers.1.adapter_layer_ff.linear_down.bias', 'encoder.layers.0.adapter_layer_attn.linear_up.bias', 'encoder.layers.10.adapter_layer_attn.linear_down.weight', 'encoder.layers.10.adapter_layer_attn.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.linear_up.weight', 'encoder.layers.4.adapter_layer_ff.linear_down.weight', 'encoder.layers.2.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_up.weight', 'encoder.layers.1.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.linear_up.bias', 'encoder.layers.5.adapter_layer_attn.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.layernorm.bias', 'encoder.layers.1.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.weight', 'encoder.layers.9.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.linear_down.weight', 'encoder.layers.8.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_attn.linear_down.bias', 'encoder.layers.2.adapter_layer_ff.layernorm.bias', 'encoder.layers.11.adapter_layer_attn.linear_up.weight', 'encoder.layers.3.adapter_layer_ff.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
frozen:  wavlm.masked_spec_embed
frozen:  wavlm.feature_extractor.conv_layers.0.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.bias
frozen:  wavlm.feature_extractor.conv_layers.1.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.2.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.3.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.4.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.5.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.6.conv.weight
frozen:  wavlm.feature_projection.layer_norm.weight
frozen:  wavlm.feature_projection.layer_norm.bias
frozen:  wavlm.feature_projection.projection.weight
frozen:  wavlm.feature_projection.projection.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_g
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_v
frozen:  wavlm.encoder.layer_norm.weight
frozen:  wavlm.encoder.layer_norm.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.0.attention.k_proj.weight
frozen:  wavlm.encoder.layers.0.attention.k_proj.bias
frozen:  wavlm.encoder.layers.0.attention.v_proj.weight
frozen:  wavlm.encoder.layers.0.attention.v_proj.bias
frozen:  wavlm.encoder.layers.0.attention.q_proj.weight
frozen:  wavlm.encoder.layers.0.attention.q_proj.bias
frozen:  wavlm.encoder.layers.0.attention.out_proj.weight
frozen:  wavlm.encoder.layers.0.attention.out_proj.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.bias
frozen:  wavlm.encoder.layers.0.attention.rel_attn_embed.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.bias
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.1.attention.k_proj.weight
frozen:  wavlm.encoder.layers.1.attention.k_proj.bias
frozen:  wavlm.encoder.layers.1.attention.v_proj.weight
frozen:  wavlm.encoder.layers.1.attention.v_proj.bias
frozen:  wavlm.encoder.layers.1.attention.q_proj.weight
frozen:  wavlm.encoder.layers.1.attention.q_proj.bias
frozen:  wavlm.encoder.layers.1.attention.out_proj.weight
frozen:  wavlm.encoder.layers.1.attention.out_proj.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.layer_norm.bias
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.2.attention.k_proj.weight
frozen:  wavlm.encoder.layers.2.attention.k_proj.bias
frozen:  wavlm.encoder.layers.2.attention.v_proj.weight
frozen:  wavlm.encoder.layers.2.attention.v_proj.bias
frozen:  wavlm.encoder.layers.2.attention.q_proj.weight
frozen:  wavlm.encoder.layers.2.attention.q_proj.bias
frozen:  wavlm.encoder.layers.2.attention.out_proj.weight
frozen:  wavlm.encoder.layers.2.attention.out_proj.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.layer_norm.bias
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.3.attention.k_proj.weight
frozen:  wavlm.encoder.layers.3.attention.k_proj.bias
frozen:  wavlm.encoder.layers.3.attention.v_proj.weight
frozen:  wavlm.encoder.layers.3.attention.v_proj.bias
frozen:  wavlm.encoder.layers.3.attention.q_proj.weight
frozen:  wavlm.encoder.layers.3.attention.q_proj.bias
frozen:  wavlm.encoder.layers.3.attention.out_proj.weight
frozen:  wavlm.encoder.layers.3.attention.out_proj.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.layer_norm.bias
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.4.attention.k_proj.weight
frozen:  wavlm.encoder.layers.4.attention.k_proj.bias
frozen:  wavlm.encoder.layers.4.attention.v_proj.weight
frozen:  wavlm.encoder.layers.4.attention.v_proj.bias
frozen:  wavlm.encoder.layers.4.attention.q_proj.weight
frozen:  wavlm.encoder.layers.4.attention.q_proj.bias
frozen:  wavlm.encoder.layers.4.attention.out_proj.weight
frozen:  wavlm.encoder.layers.4.attention.out_proj.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.layer_norm.bias
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.5.attention.k_proj.weight
frozen:  wavlm.encoder.layers.5.attention.k_proj.bias
frozen:  wavlm.encoder.layers.5.attention.v_proj.weight
frozen:  wavlm.encoder.layers.5.attention.v_proj.bias
frozen:  wavlm.encoder.layers.5.attention.q_proj.weight
frozen:  wavlm.encoder.layers.5.attention.q_proj.bias
frozen:  wavlm.encoder.layers.5.attention.out_proj.weight
frozen:  wavlm.encoder.layers.5.attention.out_proj.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.layer_norm.bias
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.6.attention.k_proj.weight
frozen:  wavlm.encoder.layers.6.attention.k_proj.bias
frozen:  wavlm.encoder.layers.6.attention.v_proj.weight
frozen:  wavlm.encoder.layers.6.attention.v_proj.bias
frozen:  wavlm.encoder.layers.6.attention.q_proj.weight
frozen:  wavlm.encoder.layers.6.attention.q_proj.bias
frozen:  wavlm.encoder.layers.6.attention.out_proj.weight
frozen:  wavlm.encoder.layers.6.attention.out_proj.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.layer_norm.bias
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.7.attention.k_proj.weight
frozen:  wavlm.encoder.layers.7.attention.k_proj.bias
frozen:  wavlm.encoder.layers.7.attention.v_proj.weight
frozen:  wavlm.encoder.layers.7.attention.v_proj.bias
frozen:  wavlm.encoder.layers.7.attention.q_proj.weight
frozen:  wavlm.encoder.layers.7.attention.q_proj.bias
frozen:  wavlm.encoder.layers.7.attention.out_proj.weight
frozen:  wavlm.encoder.layers.7.attention.out_proj.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.layer_norm.bias
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.8.attention.k_proj.weight
frozen:  wavlm.encoder.layers.8.attention.k_proj.bias
frozen:  wavlm.encoder.layers.8.attention.v_proj.weight
frozen:  wavlm.encoder.layers.8.attention.v_proj.bias
frozen:  wavlm.encoder.layers.8.attention.q_proj.weight
frozen:  wavlm.encoder.layers.8.attention.q_proj.bias
frozen:  wavlm.encoder.layers.8.attention.out_proj.weight
frozen:  wavlm.encoder.layers.8.attention.out_proj.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.layer_norm.bias
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.9.attention.k_proj.weight
frozen:  wavlm.encoder.layers.9.attention.k_proj.bias
frozen:  wavlm.encoder.layers.9.attention.v_proj.weight
frozen:  wavlm.encoder.layers.9.attention.v_proj.bias
frozen:  wavlm.encoder.layers.9.attention.q_proj.weight
frozen:  wavlm.encoder.layers.9.attention.q_proj.bias
frozen:  wavlm.encoder.layers.9.attention.out_proj.weight
frozen:  wavlm.encoder.layers.9.attention.out_proj.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.layer_norm.bias
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.10.attention.k_proj.weight
frozen:  wavlm.encoder.layers.10.attention.k_proj.bias
frozen:  wavlm.encoder.layers.10.attention.v_proj.weight
frozen:  wavlm.encoder.layers.10.attention.v_proj.bias
frozen:  wavlm.encoder.layers.10.attention.q_proj.weight
frozen:  wavlm.encoder.layers.10.attention.q_proj.bias
frozen:  wavlm.encoder.layers.10.attention.out_proj.weight
frozen:  wavlm.encoder.layers.10.attention.out_proj.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.layer_norm.bias
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.11.attention.k_proj.weight
frozen:  wavlm.encoder.layers.11.attention.k_proj.bias
frozen:  wavlm.encoder.layers.11.attention.v_proj.weight
frozen:  wavlm.encoder.layers.11.attention.v_proj.bias
frozen:  wavlm.encoder.layers.11.attention.q_proj.weight
frozen:  wavlm.encoder.layers.11.attention.q_proj.bias
frozen:  wavlm.encoder.layers.11.attention.out_proj.weight
frozen:  wavlm.encoder.layers.11.attention.out_proj.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.layer_norm.bias
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.bias
down_param:  lm_head.weight
down_param:  lm_head.bias

count of parameters:  9560096 


count of adapter_parameters:  9498624 

  0%|          | 0/100 [00:00<?, ?it/s]
Epoch 1/100 | train |  Loss: 3.2670 WER: 1.0000
Epoch 2/100 | train |  Loss: 2.8674 WER: 1.0000
Epoch 3/100 | train |  Loss: 2.6834 WER: 0.9952
Epoch 4/100 | train |  Loss: 0.7707 WER: 0.5967
Epoch 5/100 | train |  Loss: 0.4204 WER: 0.3959
Epoch 6/100 | train |  Loss: 0.3121 WER: 0.3156
Epoch 7/100 | train |  Loss: 0.2434 WER: 0.2573
Epoch 8/100 | train |  Loss: 0.2121 WER: 0.2265
Epoch 9/100 | train |  Loss: 0.1796 WER: 0.1977
Epoch 10/100 | train |  Loss: 0.1502 WER: 0.1706
Epoch 11/100 | train |  Loss: 0.1402 WER: 0.1565
Epoch 12/100 | train |  Loss: 0.1322 WER: 0.1486
Epoch 13/100 | train |  Loss: 0.1283 WER: 0.1406
Epoch 14/100 | train |  Loss: 0.1148 WER: 0.1265
Epoch 15/100 | train |  Loss: 0.1112 WER: 0.1247
Epoch 16/100 | train |  Loss: 0.1065 WER: 0.1179
Epoch 17/100 | train |  Loss: 0.0910 WER: 0.1030
Epoch 18/100 | train |  Loss: 0.0882 WER: 0.1002
Epoch 19/100 | train |  Loss: 0.0894 WER: 0.0989
Epoch 20/100 | train |  Loss: 0.0812 WER: 0.0921
Epoch 21/100 | train |  Loss: 0.0774 WER: 0.0878
Epoch 22/100 | train |  Loss: 0.0775 WER: 0.0875
Epoch 23/100 | train |  Loss: 0.0720 WER: 0.0824
Epoch 24/100 | train |  Loss: 0.0674 WER: 0.0763
Epoch 25/100 | train |  Loss: 0.0712 WER: 0.0801
Epoch 26/100 | train |  Loss: 0.0551 WER: 0.0625
Epoch 27/100 | train |  Loss: 0.0506 WER: 0.0573
Epoch 28/100 | train |  Loss: 0.0534 WER: 0.0603
Epoch 29/100 | train |  Loss: 0.0507 WER: 0.0597
Epoch 30/100 | train |  Loss: 0.0532 WER: 0.0590
Epoch 31/100 | train |  Loss: 0.0467 WER: 0.0527
Epoch 32/100 | train |  Loss: 0.0487 WER: 0.0566
Epoch 33/100 | train |  Loss: 0.0456 WER: 0.0508
Epoch 34/100 | train |  Loss: 0.0496 WER: 0.0543
Epoch 35/100 | train |  Loss: 0.0476 WER: 0.0540
Epoch 36/100 | train |  Loss: 0.0446 WER: 0.0523
Epoch 37/100 | train |  Loss: 0.0459 WER: 0.0517
Epoch 38/100 | train |  Loss: 0.0416 WER: 0.0489
Epoch 39/100 | train |  Loss: 0.0433 WER: 0.0509
Epoch 40/100 | train |  Loss: 0.0417 WER: 0.0470
Epoch 41/100 | train |  Loss: 0.0448 WER: 0.0513
Epoch 42/100 | train |  Loss: 0.0415 WER: 0.0477
Epoch 43/100 | train |  Loss: 0.0388 WER: 0.0442
Epoch 44/100 | train |  Loss: 0.0398 WER: 0.0442
Epoch 45/100 | train |  Loss: 0.0400 WER: 0.0471
Epoch 46/100 | train |  Loss: 0.0409 WER: 0.0467
Epoch 47/100 | train |  Loss: 0.0371 WER: 0.0428
Epoch 48/100 | train |  Loss: 0.0385 WER: 0.0437
Epoch 49/100 | train |  Loss: 0.0410 WER: 0.0460
Epoch 50/100 | train |  Loss: 0.0374 WER: 0.0427
Epoch 51/100 | train |  Loss: 0.0362 WER: 0.0419
Epoch 52/100 | train |  Loss: 0.0367 WER: 0.0401
Epoch 53/100 | train |  Loss: 0.0341 WER: 0.0402
Epoch 54/100 | train |  Loss: 0.0354 WER: 0.0404
Epoch 55/100 | train |  Loss: 0.0411 WER: 0.0438
Epoch 56/100 | train |  Loss: 0.0381 WER: 0.0426
Epoch 57/100 | train |  Loss: 0.0404 WER: 0.0457
Epoch 58/100 | train |  Loss: 0.0375 WER: 0.0422
Epoch 59/100 | train |  Loss: 0.0326 WER: 0.0375
Epoch 60/100 | train |  Loss: 0.0336 WER: 0.0392
Epoch 61/100 | train |  Loss: 0.0340 WER: 0.0394
Epoch 62/100 | train |  Loss: 0.0297 WER: 0.0333
Epoch 63/100 | train |  Loss: 0.0300 WER: 0.0336
Epoch 64/100 | train |  Loss: 0.0337 WER: 0.0384
Epoch 65/100 | train |  Loss: 0.0344 WER: 0.0377
Epoch 66/100 | train |  Loss: 0.0336 WER: 0.0380
Epoch 67/100 | train |  Loss: 0.0336 WER: 0.0382
Epoch 68/100 | train |  Loss: 0.0313 WER: 0.0367
Epoch 69/100 | train |  Loss: 0.0325 WER: 0.0364
Epoch 70/100 | train |  Loss: 0.0336 WER: 0.0372
Epoch 71/100 | train |  Loss: 0.0328 WER: 0.0379
Epoch 72/100 | train |  Loss: 0.0329 WER: 0.0354
Epoch 73/100 | train |  Loss: 0.0390 WER: 0.0398
Epoch 74/100 | train |  Loss: 0.0318 WER: 0.0357
Epoch 75/100 | train |  Loss: 0.0311 WER: 0.0357
Epoch 76/100 | train |  Loss: 0.0325 WER: 0.0358
Epoch 77/100 | train |  Loss: 0.0316 WER: 0.0345
Epoch 78/100 | train |  Loss: 0.0336 WER: 0.0377
Epoch 79/100 | train |  Loss: 0.0339 WER: 0.0393
Epoch 80/100 | train |  Loss: 0.0313 WER: 0.0369
Epoch 81/100 | train |  Loss: 0.0311 WER: 0.0365
Epoch 82/100 | train |  Loss: 0.0296 WER: 0.0343
Epoch 83/100 | train |  Loss: 0.0293 WER: 0.0332
Epoch 84/100 | train |  Loss: 0.0294 WER: 0.0346
Epoch 85/100 | train |  Loss: 0.0297 WER: 0.0337
Epoch 86/100 | train |  Loss: 0.0314 WER: 0.0355
Epoch 87/100 | train |  Loss: 0.0289 WER: 0.0327
Epoch 88/100 | train |  Loss: 0.0330 WER: 0.0365
Epoch 89/100 | train |  Loss: 0.0293 WER: 0.0331
Epoch 90/100 | train |  Loss: 0.0321 WER: 0.0367
Epoch 91/100 | train |  Loss: 0.0313 WER: 0.0362
Epoch 92/100 | train |  Loss: 0.0293 WER: 0.0325
Epoch 93/100 | train |  Loss: 0.0281 WER: 0.0307
Epoch 94/100 | train |  Loss: 0.0315 WER: 0.0355
Epoch 95/100 | train |  Loss: 0.0290 WER: 0.0332
Epoch 96/100 | train |  Loss: 0.0264 WER: 0.0313
Epoch 97/100 | train |  Loss: 0.0285 WER: 0.0330
Epoch 98/100 | train |  Loss: 0.0305 WER: 0.0338
Epoch 99/100 | train |  Loss: 0.0299 WER: 0.0341
Epoch 100/100 | train |  Loss: 0.0317 WER: 0.0365
Epoch 100/100 |  val  |  Loss: 0.0343 WER: 0.0914
Traceback (most recent call last):
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 318, in <module>
    main()
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 309, in main
    torch.save(model.module.state_dict(), args.run_name+'.pth')
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'AdaWavLMForCTC' object has no attribute 'module'
 
---------- Step 1: Processing complete  ----------------------
