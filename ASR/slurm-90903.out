 
---------- Step 0: Installing libraries  --------------
 
Collecting numpy==1.23.1
  Downloading numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)
Downloading numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 89.1 MB/s eta 0:00:00
Installing collected packages: numpy
Successfully installed numpy-1.23.1
---------- Step 1: Running model ----------------------
/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Some weights of AdaWavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.layers.2.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.linear_down.bias', 'encoder.layers.10.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_ff.layernorm.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.bias', 'encoder.layers.9.adapter_layer_ff.layernorm.bias', 'encoder.layers.2.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_attn.layernorm.weight', 'encoder.layers.1.adapter_layer_ff.linear_up.bias', 'encoder.layers.3.adapter_layer_ff.linear_up.weight', 'encoder.layers.0.adapter_layer_attn.linear_up.weight', 'encoder.layers.0.adapter_layer_attn.layernorm.weight', 'encoder.layers.0.adapter_layer_attn.linear_down.bias', 'encoder.layers.0.adapter_layer_attn.layernorm.bias', 'encoder.layers.3.adapter_layer_attn.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.linear_down.bias', 'encoder.layers.7.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_ff.layernorm.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.weight', 'encoder.layers.8.adapter_layer_attn.layernorm.bias', 'encoder.layers.9.adapter_layer_attn.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.layernorm.bias', 'encoder.layers.2.adapter_layer_ff.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.layernorm.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.bias', 'encoder.layers.8.adapter_layer_attn.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.linear_up.bias', 'encoder.layers.9.adapter_layer_ff.linear_down.bias', 'encoder.layers.5.adapter_layer_ff.linear_up.weight', 'encoder.layers.11.adapter_layer_ff.layernorm.weight', 'encoder.layers.11.adapter_layer_ff.linear_down.weight', 'lm_head.weight', 'encoder.layers.8.adapter_layer_ff.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.linear_up.bias', 'encoder.layers.4.adapter_layer_attn.linear_up.bias', 'encoder.layers.6.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_ff.linear_down.weight', 'encoder.layers.9.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_attn.linear_up.bias', 'encoder.layers.9.adapter_layer_attn.linear_up.weight', 'encoder.layers.7.adapter_layer_ff.linear_up.bias', 'encoder.layers.10.adapter_layer_ff.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.layernorm.bias', 'encoder.layers.4.adapter_layer_attn.linear_up.weight', 'encoder.layers.2.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_ff.layernorm.weight', 'encoder.layers.0.adapter_layer_ff.linear_down.weight', 'encoder.layers.11.adapter_layer_ff.linear_up.weight', 'lm_head.bias', 'encoder.layers.6.adapter_layer_attn.linear_down.weight', 'encoder.layers.3.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_up.weight', 'encoder.layers.6.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.layernorm.weight', 'encoder.layers.8.adapter_layer_attn.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_attn.linear_up.bias', 'encoder.layers.5.adapter_layer_ff.linear_down.bias', 'encoder.layers.2.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_ff.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_attn.linear_up.weight', 'encoder.layers.2.adapter_layer_ff.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_attn.linear_up.weight', 'encoder.layers.1.adapter_layer_attn.layernorm.bias', 'encoder.layers.4.adapter_layer_attn.layernorm.weight', 'encoder.layers.3.adapter_layer_ff.layernorm.bias', 'encoder.layers.8.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.weight', 'encoder.layers.0.adapter_layer_ff.linear_up.bias', 'encoder.layers.3.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.layernorm.weight', 'encoder.layers.4.adapter_layer_attn.linear_down.bias', 'encoder.layers.5.adapter_layer_ff.layernorm.bias', 'encoder.layers.5.adapter_layer_ff.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.linear_up.weight', 'encoder.layers.5.adapter_layer_attn.layernorm.weight', 'encoder.layers.1.adapter_layer_attn.linear_up.bias', 'encoder.layers.3.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.linear_up.weight', 'encoder.layers.11.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_up.weight', 'encoder.layers.2.adapter_layer_attn.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.weight', 'encoder.layers.7.adapter_layer_attn.linear_down.bias', 'encoder.layers.7.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.linear_down.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.bias', 'encoder.layers.5.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.weight', 'encoder.layers.10.adapter_layer_attn.linear_down.weight', 'encoder.layers.8.adapter_layer_attn.linear_up.weight', 'encoder.layers.7.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_ff.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.linear_down.weight', 'encoder.layers.3.adapter_layer_attn.linear_down.bias', 'encoder.layers.0.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_down.bias', 'encoder.layers.1.adapter_layer_attn.linear_down.bias', 'encoder.layers.5.adapter_layer_attn.linear_down.weight', 'encoder.layers.1.adapter_layer_ff.linear_up.weight', 'encoder.layers.7.adapter_layer_attn.linear_up.bias', 'encoder.layers.5.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_attn.linear_up.bias', 'encoder.layers.3.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_attn.linear_up.weight', 'encoder.layers.9.adapter_layer_attn.linear_down.weight', 'encoder.layers.7.adapter_layer_ff.layernorm.bias', 'encoder.layers.10.adapter_layer_attn.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.linear_down.weight', 'encoder.layers.6.adapter_layer_attn.linear_down.bias', 'encoder.layers.9.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_attn.linear_down.weight', 'encoder.layers.11.adapter_layer_ff.linear_up.bias', 'encoder.layers.11.adapter_layer_ff.linear_down.bias', 'encoder.layers.6.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_ff.linear_up.bias', 'encoder.layers.9.adapter_layer_ff.linear_down.weight', 'encoder.layers.0.adapter_layer_ff.linear_down.bias', 'encoder.layers.1.adapter_layer_attn.layernorm.weight', 'encoder.layers.4.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_attn.linear_up.weight', 'encoder.layers.11.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_ff.linear_up.weight', 'encoder.layers.4.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_attn.linear_up.weight', 'encoder.layers.5.adapter_layer_ff.linear_up.bias', 'encoder.layers.1.adapter_layer_ff.layernorm.bias', 'encoder.layers.2.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.layernorm.weight', 'encoder.layers.3.adapter_layer_attn.linear_up.bias', 'encoder.layers.1.adapter_layer_attn.linear_down.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
frozen:  wavlm.masked_spec_embed
frozen:  wavlm.feature_extractor.conv_layers.0.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.bias
frozen:  wavlm.feature_extractor.conv_layers.1.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.2.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.3.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.4.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.5.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.6.conv.weight
frozen:  wavlm.feature_projection.layer_norm.weight
frozen:  wavlm.feature_projection.layer_norm.bias
frozen:  wavlm.feature_projection.projection.weight
frozen:  wavlm.feature_projection.projection.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_g
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_v
frozen:  wavlm.encoder.layer_norm.weight
frozen:  wavlm.encoder.layer_norm.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.0.attention.k_proj.weight
frozen:  wavlm.encoder.layers.0.attention.k_proj.bias
frozen:  wavlm.encoder.layers.0.attention.v_proj.weight
frozen:  wavlm.encoder.layers.0.attention.v_proj.bias
frozen:  wavlm.encoder.layers.0.attention.q_proj.weight
frozen:  wavlm.encoder.layers.0.attention.q_proj.bias
frozen:  wavlm.encoder.layers.0.attention.out_proj.weight
frozen:  wavlm.encoder.layers.0.attention.out_proj.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.bias
frozen:  wavlm.encoder.layers.0.attention.rel_attn_embed.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.bias
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.1.attention.k_proj.weight
frozen:  wavlm.encoder.layers.1.attention.k_proj.bias
frozen:  wavlm.encoder.layers.1.attention.v_proj.weight
frozen:  wavlm.encoder.layers.1.attention.v_proj.bias
frozen:  wavlm.encoder.layers.1.attention.q_proj.weight
frozen:  wavlm.encoder.layers.1.attention.q_proj.bias
frozen:  wavlm.encoder.layers.1.attention.out_proj.weight
frozen:  wavlm.encoder.layers.1.attention.out_proj.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.layer_norm.bias
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.2.attention.k_proj.weight
frozen:  wavlm.encoder.layers.2.attention.k_proj.bias
frozen:  wavlm.encoder.layers.2.attention.v_proj.weight
frozen:  wavlm.encoder.layers.2.attention.v_proj.bias
frozen:  wavlm.encoder.layers.2.attention.q_proj.weight
frozen:  wavlm.encoder.layers.2.attention.q_proj.bias
frozen:  wavlm.encoder.layers.2.attention.out_proj.weight
frozen:  wavlm.encoder.layers.2.attention.out_proj.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.layer_norm.bias
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.3.attention.k_proj.weight
frozen:  wavlm.encoder.layers.3.attention.k_proj.bias
frozen:  wavlm.encoder.layers.3.attention.v_proj.weight
frozen:  wavlm.encoder.layers.3.attention.v_proj.bias
frozen:  wavlm.encoder.layers.3.attention.q_proj.weight
frozen:  wavlm.encoder.layers.3.attention.q_proj.bias
frozen:  wavlm.encoder.layers.3.attention.out_proj.weight
frozen:  wavlm.encoder.layers.3.attention.out_proj.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.layer_norm.bias
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.4.attention.k_proj.weight
frozen:  wavlm.encoder.layers.4.attention.k_proj.bias
frozen:  wavlm.encoder.layers.4.attention.v_proj.weight
frozen:  wavlm.encoder.layers.4.attention.v_proj.bias
frozen:  wavlm.encoder.layers.4.attention.q_proj.weight
frozen:  wavlm.encoder.layers.4.attention.q_proj.bias
frozen:  wavlm.encoder.layers.4.attention.out_proj.weight
frozen:  wavlm.encoder.layers.4.attention.out_proj.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.layer_norm.bias
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.5.attention.k_proj.weight
frozen:  wavlm.encoder.layers.5.attention.k_proj.bias
frozen:  wavlm.encoder.layers.5.attention.v_proj.weight
frozen:  wavlm.encoder.layers.5.attention.v_proj.bias
frozen:  wavlm.encoder.layers.5.attention.q_proj.weight
frozen:  wavlm.encoder.layers.5.attention.q_proj.bias
frozen:  wavlm.encoder.layers.5.attention.out_proj.weight
frozen:  wavlm.encoder.layers.5.attention.out_proj.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.layer_norm.bias
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.6.attention.k_proj.weight
frozen:  wavlm.encoder.layers.6.attention.k_proj.bias
frozen:  wavlm.encoder.layers.6.attention.v_proj.weight
frozen:  wavlm.encoder.layers.6.attention.v_proj.bias
frozen:  wavlm.encoder.layers.6.attention.q_proj.weight
frozen:  wavlm.encoder.layers.6.attention.q_proj.bias
frozen:  wavlm.encoder.layers.6.attention.out_proj.weight
frozen:  wavlm.encoder.layers.6.attention.out_proj.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.layer_norm.bias
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.7.attention.k_proj.weight
frozen:  wavlm.encoder.layers.7.attention.k_proj.bias
frozen:  wavlm.encoder.layers.7.attention.v_proj.weight
frozen:  wavlm.encoder.layers.7.attention.v_proj.bias
frozen:  wavlm.encoder.layers.7.attention.q_proj.weight
frozen:  wavlm.encoder.layers.7.attention.q_proj.bias
frozen:  wavlm.encoder.layers.7.attention.out_proj.weight
frozen:  wavlm.encoder.layers.7.attention.out_proj.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.layer_norm.bias
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.8.attention.k_proj.weight
frozen:  wavlm.encoder.layers.8.attention.k_proj.bias
frozen:  wavlm.encoder.layers.8.attention.v_proj.weight
frozen:  wavlm.encoder.layers.8.attention.v_proj.bias
frozen:  wavlm.encoder.layers.8.attention.q_proj.weight
frozen:  wavlm.encoder.layers.8.attention.q_proj.bias
frozen:  wavlm.encoder.layers.8.attention.out_proj.weight
frozen:  wavlm.encoder.layers.8.attention.out_proj.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.layer_norm.bias
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.9.attention.k_proj.weight
frozen:  wavlm.encoder.layers.9.attention.k_proj.bias
frozen:  wavlm.encoder.layers.9.attention.v_proj.weight
frozen:  wavlm.encoder.layers.9.attention.v_proj.bias
frozen:  wavlm.encoder.layers.9.attention.q_proj.weight
frozen:  wavlm.encoder.layers.9.attention.q_proj.bias
frozen:  wavlm.encoder.layers.9.attention.out_proj.weight
frozen:  wavlm.encoder.layers.9.attention.out_proj.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.layer_norm.bias
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.10.attention.k_proj.weight
frozen:  wavlm.encoder.layers.10.attention.k_proj.bias
frozen:  wavlm.encoder.layers.10.attention.v_proj.weight
frozen:  wavlm.encoder.layers.10.attention.v_proj.bias
frozen:  wavlm.encoder.layers.10.attention.q_proj.weight
frozen:  wavlm.encoder.layers.10.attention.q_proj.bias
frozen:  wavlm.encoder.layers.10.attention.out_proj.weight
frozen:  wavlm.encoder.layers.10.attention.out_proj.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.layer_norm.bias
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.11.attention.k_proj.weight
frozen:  wavlm.encoder.layers.11.attention.k_proj.bias
frozen:  wavlm.encoder.layers.11.attention.v_proj.weight
frozen:  wavlm.encoder.layers.11.attention.v_proj.bias
frozen:  wavlm.encoder.layers.11.attention.q_proj.weight
frozen:  wavlm.encoder.layers.11.attention.q_proj.bias
frozen:  wavlm.encoder.layers.11.attention.out_proj.weight
frozen:  wavlm.encoder.layers.11.attention.out_proj.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.layer_norm.bias
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.bias
down_param:  lm_head.weight
down_param:  lm_head.bias

count of parameters:  9560096 


count of adapter_parameters:  9498624 

  0%|          | 0/100 [00:00<?, ?it/s]
/home/skscla001/.local/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py:145: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=np.bool)
Traceback (most recent call last):
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 318, in <module>
    main()
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 307, in main
    model = train_model(model, processor, dataloaders_dict, optimizer, scheduler, metric, num_epochs, report_wandb=False, val_interval=100)
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/utils.py", line 164, in train_model
    outputs = model(**inputs)
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/skscla001/.local/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1349, in forward
    outputs = self.wavlm(
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/skscla001/.local/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1247, in forward
    hidden_states = self._mask_hidden_states(
  File "/home/skscla001/.local/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1189, in _mask_hidden_states
    mask_time_indices = _compute_mask_indices(
  File "/home/skscla001/.local/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 145, in _compute_mask_indices
    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=np.bool)
  File "/opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
 
---------- Step 1: Processing complete  ----------------------
