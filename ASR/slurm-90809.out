 
---------- Step 0: Installing libraries  --------------
 
Channels:
 - conda-forge
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done

## Package Plan ##

  environment location: /home/skscla001/.conda/envs/wavlm



Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
#
# To activate this environment, use
#
#     $ conda activate wavlm
#
# To deactivate an active environment, use
#
#     $ conda deactivate

Collecting torch
  Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)
Collecting torchvision
  Downloading torchvision-0.21.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.1 kB)
Collecting torchaudio
  Downloading torchaudio-2.6.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)
Collecting transformers
  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)
Collecting datasets
  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)
Collecting huggingface_hub
  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)
Collecting speechbrain
  Downloading speechbrain-1.0.2-py3-none-any.whl.metadata (23 kB)
Collecting evaluate
  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)
Collecting jiwer
  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)
Collecting filelock (from torch)
  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)
Collecting typing-extensions>=4.10.0 (from torch)
  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting networkx (from torch)
  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)
Collecting jinja2 (from torch)
  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)
Collecting fsspec (from torch)
  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)
  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)
  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)
  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)
  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)
  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)
  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.5.147 (from torch)
  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)
  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)
  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)
  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting nvidia-nccl-cu12==2.21.5 (from torch)
  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvtx-cu12==12.4.127 (from torch)
  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)
  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting triton==3.2.0 (from torch)
  Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)
Collecting sympy==1.13.1 (from torch)
  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)
  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Requirement already satisfied: numpy in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from torchvision) (1.26.4)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from torchvision) (10.4.0)
Requirement already satisfied: packaging>=20.0 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from transformers) (24.1)
Collecting pyyaml>=5.1 (from transformers)
  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting regex!=2019.12.17 (from transformers)
  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Requirement already satisfied: requests in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from transformers) (2.32.3)
Collecting tokenizers<0.22,>=0.21 (from transformers)
  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting safetensors>=0.4.1 (from transformers)
  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Requirement already satisfied: tqdm>=4.27 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from transformers) (4.66.4)
Collecting pyarrow>=15.0.0 (from datasets)
  Downloading pyarrow-19.0.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting dill<0.3.9,>=0.3.0 (from datasets)
  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)
Collecting pandas (from datasets)
  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
Collecting xxhash (from datasets)
  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting multiprocess<0.70.17 (from datasets)
  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)
Collecting fsspec (from torch)
  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)
Collecting aiohttp (from datasets)
  Downloading aiohttp-3.11.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting hyperpyyaml (from speechbrain)
  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)
Collecting joblib (from speechbrain)
  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)
Collecting scipy (from speechbrain)
  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
Collecting sentencepiece (from speechbrain)
  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting click>=8.1.8 (from jiwer)
  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)
Collecting rapidfuzz>=3.9.7 (from jiwer)
  Downloading rapidfuzz-3.12.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)
  Downloading aiohappyeyeballs-2.4.8-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.1.2 (from aiohttp->datasets)
  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)
Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)
  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)
Collecting attrs>=17.3.0 (from aiohttp->datasets)
  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)
Collecting frozenlist>=1.1.1 (from aiohttp->datasets)
  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)
  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)
Collecting propcache>=0.2.0 (from aiohttp->datasets)
  Downloading propcache-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)
  Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests->transformers) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)
Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)
  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch)
  Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Requirement already satisfied: python-dateutil>=2.8.2 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Collecting pytz>=2020.1 (from pandas->datasets)
  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->datasets)
  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)
Requirement already satisfied: six>=1.5 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)
Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)
  Downloading ruamel.yaml.clib-0.2.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)
Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl (766.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 766.7/766.7 MB 83.2 MB/s eta 0:00:00
Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 121.6 MB/s eta 0:00:00
Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 104.2 MB/s eta 0:00:00
Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 118.3 MB/s eta 0:00:00
Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 35.1 MB/s eta 0:00:00
Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 100.9 MB/s eta 0:00:00
Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 103.2 MB/s eta 0:00:00
Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 123.1 MB/s eta 0:00:00
Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 129.2 MB/s eta 0:00:00
Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 133.1 MB/s eta 0:00:00
Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.1/150.1 MB 132.0 MB/s eta 0:00:00
Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 133.9 MB/s eta 0:00:00
Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 124.9 MB/s eta 0:00:00
Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)
Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 100.8 MB/s eta 0:00:00
Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.1/253.1 MB 131.9 MB/s eta 0:00:00
Downloading torchvision-0.21.0-cp39-cp39-manylinux1_x86_64.whl (7.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 85.0 MB/s eta 0:00:00
Downloading torchaudio-2.6.0-cp39-cp39-manylinux1_x86_64.whl (3.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 80.5 MB/s eta 0:00:00
Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 105.4 MB/s eta 0:00:00
Downloading datasets-3.3.2-py3-none-any.whl (485 kB)
Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)
Downloading speechbrain-1.0.2-py3-none-any.whl (824 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 824.8/824.8 kB 27.1 MB/s eta 0:00:00
Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)
Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)
Downloading click-8.1.8-py3-none-any.whl (98 kB)
Downloading dill-0.3.8-py3-none-any.whl (116 kB)
Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)
Downloading aiohttp-3.11.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 56.0 MB/s eta 0:00:00
Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)
Downloading pyarrow-19.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (42.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 125.4 MB/s eta 0:00:00
Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 737.4/737.4 kB 26.7 MB/s eta 0:00:00
Downloading rapidfuzz-3.12.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 76.1 MB/s eta 0:00:00
Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 780.9/780.9 kB 27.8 MB/s eta 0:00:00
Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 75.3 MB/s eta 0:00:00
Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Downloading filelock-3.17.0-py3-none-any.whl (16 kB)
Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)
Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)
Downloading joblib-1.4.2-py3-none-any.whl (301 kB)
Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 58.6 MB/s eta 0:00:00
Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 114.2 MB/s eta 0:00:00
Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 116.8 MB/s eta 0:00:00
Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 53.2 MB/s eta 0:00:00
Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)
Downloading aiohappyeyeballs-2.4.8-py3-none-any.whl (15 kB)
Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)
Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)
Downloading attrs-25.1.0-py3-none-any.whl (63 kB)
Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)
Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)
Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 17.9 MB/s eta 0:00:00
Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)
Downloading propcache-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)
Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)
Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)
Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)
Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)
Downloading ruamel.yaml.clib-0.2.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 724.9/724.9 kB 29.9 MB/s eta 0:00:00
Installing collected packages: triton, sentencepiece, pytz, nvidia-cusparselt-cu12, mpmath, xxhash, tzdata, typing-extensions, sympy, scipy, safetensors, ruamel.yaml.clib, regex, rapidfuzz, pyyaml, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, fsspec, frozenlist, filelock, dill, click, attrs, async-timeout, aiohappyeyeballs, ruamel.yaml, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, multidict, jiwer, jinja2, huggingface_hub, aiosignal, yarl, tokenizers, nvidia-cusolver-cu12, hyperpyyaml, transformers, torch, aiohttp, torchvision, torchaudio, speechbrain, datasets, evaluate
Successfully installed MarkupSafe-3.0.2 aiohappyeyeballs-2.4.8 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 click-8.1.8 datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 filelock-3.17.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface_hub-0.29.1 hyperpyyaml-1.2.2 jinja2-3.1.5 jiwer-3.1.0 joblib-1.4.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.2.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 pyyaml-6.0.2 rapidfuzz-3.12.2 regex-2024.11.6 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 safetensors-0.5.3 scipy-1.13.1 sentencepiece-0.2.0 speechbrain-1.0.2 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 transformers-4.49.0 triton-3.2.0 typing-extensions-4.12.2 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3
Collecting wandb
  Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: click!=8.0.0,>=7.1 in /scratch/skscla001/.local/lib/python3.9/site-packages (from wandb) (8.1.8)
Collecting docker-pycreds>=0.4.0 (from wandb)
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting eval-type-backport (from wandb)
  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)
Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)
  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: platformdirs in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from wandb) (3.10.0)
Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 (from wandb)
  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)
Collecting psutil>=5.0.0 (from wandb)
  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pydantic<3,>=2.6 (from wandb)
  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)
Requirement already satisfied: pyyaml in /scratch/skscla001/.local/lib/python3.9/site-packages (from wandb) (6.0.2)
Requirement already satisfied: requests<3,>=2.0.0 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from wandb) (2.32.3)
Collecting sentry-sdk>=2.0.0 (from wandb)
  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl.metadata (10 kB)
Collecting setproctitle (from wandb)
  Downloading setproctitle-1.3.5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: setuptools in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from wandb) (72.1.0)
Requirement already satisfied: typing-extensions<5,>=4.4 in /scratch/skscla001/.local/lib/python3.9/site-packages (from wandb) (4.12.2)
Requirement already satisfied: six>=1.4.0 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)
  Downloading pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/20.8 MB 87.5 MB/s eta 0:00:00
Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)
Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)
Downloading pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 58.3 MB/s eta 0:00:00
Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)
Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)
Downloading setproctitle-1.3.5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)
Downloading smmap-5.0.2-py3-none-any.whl (24 kB)
Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, psutil, protobuf, eval-type-backport, docker-pycreds, annotated-types, pydantic, gitdb, gitpython, wandb
Successfully installed annotated-types-0.7.0 docker-pycreds-0.4.0 eval-type-backport-0.2.2 gitdb-4.0.12 gitpython-3.1.44 protobuf-5.29.3 psutil-7.0.0 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.7
Requirement already satisfied: tqdm in /opt/exp_soft/miniconda3-py3.9/lib/python3.9/site-packages (4.66.4)
Collecting ipywidgets
  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)
Collecting comm>=0.1.3 (from ipywidgets)
  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)
Collecting ipython>=6.1.0 (from ipywidgets)
  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)
Collecting traitlets>=4.3.1 (from ipywidgets)
  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting widgetsnbextension~=4.0.12 (from ipywidgets)
  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)
Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)
  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)
Collecting decorator (from ipython>=6.1.0->ipywidgets)
  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)
  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting matplotlib-inline (from ipython>=6.1.0->ipywidgets)
  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)
Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=6.1.0->ipywidgets)
  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)
Collecting pygments>=2.4.0 (from ipython>=6.1.0->ipywidgets)
  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)
Collecting stack-data (from ipython>=6.1.0->ipywidgets)
  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)
Requirement already satisfied: typing-extensions in /scratch/skscla001/.local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)
Collecting exceptiongroup (from ipython>=6.1.0->ipywidgets)
  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)
Collecting pexpect>4.3 (from ipython>=6.1.0->ipywidgets)
  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)
Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=6.1.0->ipywidgets)
  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)
Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=6.1.0->ipywidgets)
  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets)
  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)
Collecting executing>=1.2.0 (from stack-data->ipython>=6.1.0->ipywidgets)
  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)
Collecting asttokens>=2.1.0 (from stack-data->ipython>=6.1.0->ipywidgets)
  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)
Collecting pure-eval (from stack-data->ipython>=6.1.0->ipywidgets)
  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)
Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)
Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)
Downloading ipython-8.18.1-py3-none-any.whl (808 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 808.2/808.2 kB 32.2 MB/s eta 0:00:00
Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)
Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)
Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 64.2 MB/s eta 0:00:00
Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 52.7 MB/s eta 0:00:00
Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)
Downloading prompt_toolkit-3.0.50-py3-none-any.whl (387 kB)
Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 45.1 MB/s eta 0:00:00
Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)
Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)
Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)
Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)
Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)
Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)
Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)
Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Installing collected packages: wcwidth, pure-eval, ptyprocess, widgetsnbextension, traitlets, pygments, prompt-toolkit, pexpect, parso, jupyterlab-widgets, executing, exceptiongroup, decorator, asttokens, stack-data, matplotlib-inline, jedi, comm, ipython, ipywidgets
Successfully installed asttokens-3.0.0 comm-0.2.2 decorator-5.2.1 exceptiongroup-1.2.2 executing-2.2.0 ipython-8.18.1 ipywidgets-8.1.5 jedi-0.19.2 jupyterlab-widgets-3.0.13 matplotlib-inline-0.1.7 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.50 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.1 stack-data-0.6.3 traitlets-5.14.3 wcwidth-0.2.13 widgetsnbextension-4.0.13
---------- Step 1: Running model ----------------------
/scratch/skscla001/speech/adapter-wavlm/ASR/utils.py:9: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("sox_io")
Some weights of AdaWavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.layers.0.adapter_layer_attn.layernorm.bias', 'encoder.layers.0.adapter_layer_attn.layernorm.weight', 'encoder.layers.0.adapter_layer_attn.linear_down.bias', 'encoder.layers.0.adapter_layer_attn.linear_down.weight', 'encoder.layers.0.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_attn.linear_up.weight', 'encoder.layers.0.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.weight', 'encoder.layers.0.adapter_layer_ff.linear_down.bias', 'encoder.layers.0.adapter_layer_ff.linear_down.weight', 'encoder.layers.0.adapter_layer_ff.linear_up.bias', 'encoder.layers.0.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_attn.layernorm.bias', 'encoder.layers.1.adapter_layer_attn.layernorm.weight', 'encoder.layers.1.adapter_layer_attn.linear_down.bias', 'encoder.layers.1.adapter_layer_attn.linear_down.weight', 'encoder.layers.1.adapter_layer_attn.linear_up.bias', 'encoder.layers.1.adapter_layer_attn.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.layernorm.bias', 'encoder.layers.1.adapter_layer_ff.layernorm.weight', 'encoder.layers.1.adapter_layer_ff.linear_down.bias', 'encoder.layers.1.adapter_layer_ff.linear_down.weight', 'encoder.layers.1.adapter_layer_ff.linear_up.bias', 'encoder.layers.1.adapter_layer_ff.linear_up.weight', 'encoder.layers.10.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_attn.layernorm.weight', 'encoder.layers.10.adapter_layer_attn.linear_down.bias', 'encoder.layers.10.adapter_layer_attn.linear_down.weight', 'encoder.layers.10.adapter_layer_attn.linear_up.bias', 'encoder.layers.10.adapter_layer_attn.linear_up.weight', 'encoder.layers.10.adapter_layer_ff.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.layernorm.weight', 'encoder.layers.10.adapter_layer_ff.linear_down.bias', 'encoder.layers.10.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_up.bias', 'encoder.layers.10.adapter_layer_ff.linear_up.weight', 'encoder.layers.11.adapter_layer_attn.layernorm.bias', 'encoder.layers.11.adapter_layer_attn.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.linear_down.weight', 'encoder.layers.11.adapter_layer_attn.linear_up.bias', 'encoder.layers.11.adapter_layer_attn.linear_up.weight', 'encoder.layers.11.adapter_layer_ff.layernorm.bias', 'encoder.layers.11.adapter_layer_ff.layernorm.weight', 'encoder.layers.11.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_ff.linear_down.weight', 'encoder.layers.11.adapter_layer_ff.linear_up.bias', 'encoder.layers.11.adapter_layer_ff.linear_up.weight', 'encoder.layers.2.adapter_layer_attn.layernorm.bias', 'encoder.layers.2.adapter_layer_attn.layernorm.weight', 'encoder.layers.2.adapter_layer_attn.linear_down.bias', 'encoder.layers.2.adapter_layer_attn.linear_down.weight', 'encoder.layers.2.adapter_layer_attn.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.linear_up.weight', 'encoder.layers.2.adapter_layer_ff.layernorm.bias', 'encoder.layers.2.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_ff.linear_down.bias', 'encoder.layers.2.adapter_layer_ff.linear_down.weight', 'encoder.layers.2.adapter_layer_ff.linear_up.bias', 'encoder.layers.2.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.layernorm.bias', 'encoder.layers.3.adapter_layer_attn.layernorm.weight', 'encoder.layers.3.adapter_layer_attn.linear_down.bias', 'encoder.layers.3.adapter_layer_attn.linear_down.weight', 'encoder.layers.3.adapter_layer_attn.linear_up.bias', 'encoder.layers.3.adapter_layer_attn.linear_up.weight', 'encoder.layers.3.adapter_layer_ff.layernorm.bias', 'encoder.layers.3.adapter_layer_ff.layernorm.weight', 'encoder.layers.3.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_ff.linear_down.weight', 'encoder.layers.3.adapter_layer_ff.linear_up.bias', 'encoder.layers.3.adapter_layer_ff.linear_up.weight', 'encoder.layers.4.adapter_layer_attn.layernorm.bias', 'encoder.layers.4.adapter_layer_attn.layernorm.weight', 'encoder.layers.4.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_attn.linear_down.weight', 'encoder.layers.4.adapter_layer_attn.linear_up.bias', 'encoder.layers.4.adapter_layer_attn.linear_up.weight', 'encoder.layers.4.adapter_layer_ff.layernorm.bias', 'encoder.layers.4.adapter_layer_ff.layernorm.weight', 'encoder.layers.4.adapter_layer_ff.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.linear_up.bias', 'encoder.layers.4.adapter_layer_ff.linear_up.weight', 'encoder.layers.5.adapter_layer_attn.layernorm.bias', 'encoder.layers.5.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_down.bias', 'encoder.layers.5.adapter_layer_attn.linear_down.weight', 'encoder.layers.5.adapter_layer_attn.linear_up.bias', 'encoder.layers.5.adapter_layer_attn.linear_up.weight', 'encoder.layers.5.adapter_layer_ff.layernorm.bias', 'encoder.layers.5.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_ff.linear_down.bias', 'encoder.layers.5.adapter_layer_ff.linear_down.weight', 'encoder.layers.5.adapter_layer_ff.linear_up.bias', 'encoder.layers.5.adapter_layer_ff.linear_up.weight', 'encoder.layers.6.adapter_layer_attn.layernorm.bias', 'encoder.layers.6.adapter_layer_attn.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.linear_down.bias', 'encoder.layers.6.adapter_layer_attn.linear_down.weight', 'encoder.layers.6.adapter_layer_attn.linear_up.bias', 'encoder.layers.6.adapter_layer_attn.linear_up.weight', 'encoder.layers.6.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_ff.layernorm.weight', 'encoder.layers.6.adapter_layer_ff.linear_down.bias', 'encoder.layers.6.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_ff.linear_up.bias', 'encoder.layers.6.adapter_layer_ff.linear_up.weight', 'encoder.layers.7.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.weight', 'encoder.layers.7.adapter_layer_attn.linear_down.bias', 'encoder.layers.7.adapter_layer_attn.linear_down.weight', 'encoder.layers.7.adapter_layer_attn.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.linear_up.weight', 'encoder.layers.7.adapter_layer_ff.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.layernorm.weight', 'encoder.layers.7.adapter_layer_ff.linear_down.bias', 'encoder.layers.7.adapter_layer_ff.linear_down.weight', 'encoder.layers.7.adapter_layer_ff.linear_up.bias', 'encoder.layers.7.adapter_layer_ff.linear_up.weight', 'encoder.layers.8.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_attn.layernorm.weight', 'encoder.layers.8.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_attn.linear_down.weight', 'encoder.layers.8.adapter_layer_attn.linear_up.bias', 'encoder.layers.8.adapter_layer_attn.linear_up.weight', 'encoder.layers.8.adapter_layer_ff.layernorm.bias', 'encoder.layers.8.adapter_layer_ff.layernorm.weight', 'encoder.layers.8.adapter_layer_ff.linear_down.bias', 'encoder.layers.8.adapter_layer_ff.linear_down.weight', 'encoder.layers.8.adapter_layer_ff.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.weight', 'encoder.layers.9.adapter_layer_attn.layernorm.bias', 'encoder.layers.9.adapter_layer_attn.layernorm.weight', 'encoder.layers.9.adapter_layer_attn.linear_down.bias', 'encoder.layers.9.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_attn.linear_up.bias', 'encoder.layers.9.adapter_layer_attn.linear_up.weight', 'encoder.layers.9.adapter_layer_ff.layernorm.bias', 'encoder.layers.9.adapter_layer_ff.layernorm.weight', 'encoder.layers.9.adapter_layer_ff.linear_down.bias', 'encoder.layers.9.adapter_layer_ff.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.bias', 'encoder.layers.9.adapter_layer_ff.linear_up.weight', 'lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2.6.0+cu124
2.6.0+cu124
frozen:  wavlm.masked_spec_embed
frozen:  wavlm.feature_extractor.conv_layers.0.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.bias
frozen:  wavlm.feature_extractor.conv_layers.1.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.2.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.3.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.4.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.5.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.6.conv.weight
frozen:  wavlm.feature_projection.layer_norm.weight
frozen:  wavlm.feature_projection.layer_norm.bias
frozen:  wavlm.feature_projection.projection.weight
frozen:  wavlm.feature_projection.projection.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0
frozen:  wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1
frozen:  wavlm.encoder.layer_norm.weight
frozen:  wavlm.encoder.layer_norm.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.0.attention.k_proj.weight
frozen:  wavlm.encoder.layers.0.attention.k_proj.bias
frozen:  wavlm.encoder.layers.0.attention.v_proj.weight
frozen:  wavlm.encoder.layers.0.attention.v_proj.bias
frozen:  wavlm.encoder.layers.0.attention.q_proj.weight
frozen:  wavlm.encoder.layers.0.attention.q_proj.bias
frozen:  wavlm.encoder.layers.0.attention.out_proj.weight
frozen:  wavlm.encoder.layers.0.attention.out_proj.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.bias
frozen:  wavlm.encoder.layers.0.attention.rel_attn_embed.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.bias
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.1.attention.k_proj.weight
frozen:  wavlm.encoder.layers.1.attention.k_proj.bias
frozen:  wavlm.encoder.layers.1.attention.v_proj.weight
frozen:  wavlm.encoder.layers.1.attention.v_proj.bias
frozen:  wavlm.encoder.layers.1.attention.q_proj.weight
frozen:  wavlm.encoder.layers.1.attention.q_proj.bias
frozen:  wavlm.encoder.layers.1.attention.out_proj.weight
frozen:  wavlm.encoder.layers.1.attention.out_proj.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.layer_norm.bias
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.2.attention.k_proj.weight
frozen:  wavlm.encoder.layers.2.attention.k_proj.bias
frozen:  wavlm.encoder.layers.2.attention.v_proj.weight
frozen:  wavlm.encoder.layers.2.attention.v_proj.bias
frozen:  wavlm.encoder.layers.2.attention.q_proj.weight
frozen:  wavlm.encoder.layers.2.attention.q_proj.bias
frozen:  wavlm.encoder.layers.2.attention.out_proj.weight
frozen:  wavlm.encoder.layers.2.attention.out_proj.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.layer_norm.bias
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.3.attention.k_proj.weight
frozen:  wavlm.encoder.layers.3.attention.k_proj.bias
frozen:  wavlm.encoder.layers.3.attention.v_proj.weight
frozen:  wavlm.encoder.layers.3.attention.v_proj.bias
frozen:  wavlm.encoder.layers.3.attention.q_proj.weight
frozen:  wavlm.encoder.layers.3.attention.q_proj.bias
frozen:  wavlm.encoder.layers.3.attention.out_proj.weight
frozen:  wavlm.encoder.layers.3.attention.out_proj.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.layer_norm.bias
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.4.attention.k_proj.weight
frozen:  wavlm.encoder.layers.4.attention.k_proj.bias
frozen:  wavlm.encoder.layers.4.attention.v_proj.weight
frozen:  wavlm.encoder.layers.4.attention.v_proj.bias
frozen:  wavlm.encoder.layers.4.attention.q_proj.weight
frozen:  wavlm.encoder.layers.4.attention.q_proj.bias
frozen:  wavlm.encoder.layers.4.attention.out_proj.weight
frozen:  wavlm.encoder.layers.4.attention.out_proj.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.layer_norm.bias
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.5.attention.k_proj.weight
frozen:  wavlm.encoder.layers.5.attention.k_proj.bias
frozen:  wavlm.encoder.layers.5.attention.v_proj.weight
frozen:  wavlm.encoder.layers.5.attention.v_proj.bias
frozen:  wavlm.encoder.layers.5.attention.q_proj.weight
frozen:  wavlm.encoder.layers.5.attention.q_proj.bias
frozen:  wavlm.encoder.layers.5.attention.out_proj.weight
frozen:  wavlm.encoder.layers.5.attention.out_proj.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.layer_norm.bias
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.6.attention.k_proj.weight
frozen:  wavlm.encoder.layers.6.attention.k_proj.bias
frozen:  wavlm.encoder.layers.6.attention.v_proj.weight
frozen:  wavlm.encoder.layers.6.attention.v_proj.bias
frozen:  wavlm.encoder.layers.6.attention.q_proj.weight
frozen:  wavlm.encoder.layers.6.attention.q_proj.bias
frozen:  wavlm.encoder.layers.6.attention.out_proj.weight
frozen:  wavlm.encoder.layers.6.attention.out_proj.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.layer_norm.bias
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.7.attention.k_proj.weight
frozen:  wavlm.encoder.layers.7.attention.k_proj.bias
frozen:  wavlm.encoder.layers.7.attention.v_proj.weight
frozen:  wavlm.encoder.layers.7.attention.v_proj.bias
frozen:  wavlm.encoder.layers.7.attention.q_proj.weight
frozen:  wavlm.encoder.layers.7.attention.q_proj.bias
frozen:  wavlm.encoder.layers.7.attention.out_proj.weight
frozen:  wavlm.encoder.layers.7.attention.out_proj.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.layer_norm.bias
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.8.attention.k_proj.weight
frozen:  wavlm.encoder.layers.8.attention.k_proj.bias
frozen:  wavlm.encoder.layers.8.attention.v_proj.weight
frozen:  wavlm.encoder.layers.8.attention.v_proj.bias
frozen:  wavlm.encoder.layers.8.attention.q_proj.weight
frozen:  wavlm.encoder.layers.8.attention.q_proj.bias
frozen:  wavlm.encoder.layers.8.attention.out_proj.weight
frozen:  wavlm.encoder.layers.8.attention.out_proj.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.layer_norm.bias
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.9.attention.k_proj.weight
frozen:  wavlm.encoder.layers.9.attention.k_proj.bias
frozen:  wavlm.encoder.layers.9.attention.v_proj.weight
frozen:  wavlm.encoder.layers.9.attention.v_proj.bias
frozen:  wavlm.encoder.layers.9.attention.q_proj.weight
frozen:  wavlm.encoder.layers.9.attention.q_proj.bias
frozen:  wavlm.encoder.layers.9.attention.out_proj.weight
frozen:  wavlm.encoder.layers.9.attention.out_proj.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.layer_norm.bias
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.10.attention.k_proj.weight
frozen:  wavlm.encoder.layers.10.attention.k_proj.bias
frozen:  wavlm.encoder.layers.10.attention.v_proj.weight
frozen:  wavlm.encoder.layers.10.attention.v_proj.bias
frozen:  wavlm.encoder.layers.10.attention.q_proj.weight
frozen:  wavlm.encoder.layers.10.attention.q_proj.bias
frozen:  wavlm.encoder.layers.10.attention.out_proj.weight
frozen:  wavlm.encoder.layers.10.attention.out_proj.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.layer_norm.bias
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.11.attention.k_proj.weight
frozen:  wavlm.encoder.layers.11.attention.k_proj.bias
frozen:  wavlm.encoder.layers.11.attention.v_proj.weight
frozen:  wavlm.encoder.layers.11.attention.v_proj.bias
frozen:  wavlm.encoder.layers.11.attention.q_proj.weight
frozen:  wavlm.encoder.layers.11.attention.q_proj.bias
frozen:  wavlm.encoder.layers.11.attention.out_proj.weight
frozen:  wavlm.encoder.layers.11.attention.out_proj.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.layer_norm.bias
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.bias
down_param:  lm_head.weight
down_param:  lm_head.bias

count of parameters:  9560096 


count of adapter_parameters:  9498624 

Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 22.0MB/s]
/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 318, in <module>
    main()
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/train.py", line 307, in main
    model = train_model(model, processor, dataloaders_dict, optimizer, scheduler, metric, num_epochs, report_wandb=False, val_interval=100)
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/utils.py", line 157, in train_model
    for step, inputs in enumerate(dataloaders_dict[phase]):
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1480, in _next_data
    return self._process_data(data)
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1505, in _process_data
    data.reraise()
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/_utils.py", line 733, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/scratch/skscla001/speech/adapter-wavlm/ASR/utils.py", line 32, in __getitem__
    array, sampling_rate = torchaudio.load(data['wav'])
  File "/home/skscla001/.local/lib/python3.9/site-packages/torchaudio/_backend/utils.py", line 204, in load
    backend = dispatcher(uri, format, backend)
  File "/home/skscla001/.local/lib/python3.9/site-packages/torchaudio/_backend/utils.py", line 116, in dispatcher
    raise RuntimeError(f"Couldn't find appropriate backend to handle uri {uri} and format {format}.")
RuntimeError: Couldn't find appropriate backend to handle uri ../data/librilight/librispeech_finetuning/1h/5/other/8765/295000/8765-295000-0008.flac and format None.

 
---------- Step 1: Processing complete  ----------------------
