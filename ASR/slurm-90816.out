 
---------- Step 0: Installing libraries  --------------
 
---------- Step 1: Running model ----------------------
/scratch/skscla001/speech/adapter-wavlm/ASR/utils.py:9: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("sox_io")
Some weights of AdaWavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.layers.0.adapter_layer_attn.layernorm.bias', 'encoder.layers.0.adapter_layer_attn.layernorm.weight', 'encoder.layers.0.adapter_layer_attn.linear_down.bias', 'encoder.layers.0.adapter_layer_attn.linear_down.weight', 'encoder.layers.0.adapter_layer_attn.linear_up.bias', 'encoder.layers.0.adapter_layer_attn.linear_up.weight', 'encoder.layers.0.adapter_layer_ff.layernorm.bias', 'encoder.layers.0.adapter_layer_ff.layernorm.weight', 'encoder.layers.0.adapter_layer_ff.linear_down.bias', 'encoder.layers.0.adapter_layer_ff.linear_down.weight', 'encoder.layers.0.adapter_layer_ff.linear_up.bias', 'encoder.layers.0.adapter_layer_ff.linear_up.weight', 'encoder.layers.1.adapter_layer_attn.layernorm.bias', 'encoder.layers.1.adapter_layer_attn.layernorm.weight', 'encoder.layers.1.adapter_layer_attn.linear_down.bias', 'encoder.layers.1.adapter_layer_attn.linear_down.weight', 'encoder.layers.1.adapter_layer_attn.linear_up.bias', 'encoder.layers.1.adapter_layer_attn.linear_up.weight', 'encoder.layers.1.adapter_layer_ff.layernorm.bias', 'encoder.layers.1.adapter_layer_ff.layernorm.weight', 'encoder.layers.1.adapter_layer_ff.linear_down.bias', 'encoder.layers.1.adapter_layer_ff.linear_down.weight', 'encoder.layers.1.adapter_layer_ff.linear_up.bias', 'encoder.layers.1.adapter_layer_ff.linear_up.weight', 'encoder.layers.10.adapter_layer_attn.layernorm.bias', 'encoder.layers.10.adapter_layer_attn.layernorm.weight', 'encoder.layers.10.adapter_layer_attn.linear_down.bias', 'encoder.layers.10.adapter_layer_attn.linear_down.weight', 'encoder.layers.10.adapter_layer_attn.linear_up.bias', 'encoder.layers.10.adapter_layer_attn.linear_up.weight', 'encoder.layers.10.adapter_layer_ff.layernorm.bias', 'encoder.layers.10.adapter_layer_ff.layernorm.weight', 'encoder.layers.10.adapter_layer_ff.linear_down.bias', 'encoder.layers.10.adapter_layer_ff.linear_down.weight', 'encoder.layers.10.adapter_layer_ff.linear_up.bias', 'encoder.layers.10.adapter_layer_ff.linear_up.weight', 'encoder.layers.11.adapter_layer_attn.layernorm.bias', 'encoder.layers.11.adapter_layer_attn.layernorm.weight', 'encoder.layers.11.adapter_layer_attn.linear_down.bias', 'encoder.layers.11.adapter_layer_attn.linear_down.weight', 'encoder.layers.11.adapter_layer_attn.linear_up.bias', 'encoder.layers.11.adapter_layer_attn.linear_up.weight', 'encoder.layers.11.adapter_layer_ff.layernorm.bias', 'encoder.layers.11.adapter_layer_ff.layernorm.weight', 'encoder.layers.11.adapter_layer_ff.linear_down.bias', 'encoder.layers.11.adapter_layer_ff.linear_down.weight', 'encoder.layers.11.adapter_layer_ff.linear_up.bias', 'encoder.layers.11.adapter_layer_ff.linear_up.weight', 'encoder.layers.2.adapter_layer_attn.layernorm.bias', 'encoder.layers.2.adapter_layer_attn.layernorm.weight', 'encoder.layers.2.adapter_layer_attn.linear_down.bias', 'encoder.layers.2.adapter_layer_attn.linear_down.weight', 'encoder.layers.2.adapter_layer_attn.linear_up.bias', 'encoder.layers.2.adapter_layer_attn.linear_up.weight', 'encoder.layers.2.adapter_layer_ff.layernorm.bias', 'encoder.layers.2.adapter_layer_ff.layernorm.weight', 'encoder.layers.2.adapter_layer_ff.linear_down.bias', 'encoder.layers.2.adapter_layer_ff.linear_down.weight', 'encoder.layers.2.adapter_layer_ff.linear_up.bias', 'encoder.layers.2.adapter_layer_ff.linear_up.weight', 'encoder.layers.3.adapter_layer_attn.layernorm.bias', 'encoder.layers.3.adapter_layer_attn.layernorm.weight', 'encoder.layers.3.adapter_layer_attn.linear_down.bias', 'encoder.layers.3.adapter_layer_attn.linear_down.weight', 'encoder.layers.3.adapter_layer_attn.linear_up.bias', 'encoder.layers.3.adapter_layer_attn.linear_up.weight', 'encoder.layers.3.adapter_layer_ff.layernorm.bias', 'encoder.layers.3.adapter_layer_ff.layernorm.weight', 'encoder.layers.3.adapter_layer_ff.linear_down.bias', 'encoder.layers.3.adapter_layer_ff.linear_down.weight', 'encoder.layers.3.adapter_layer_ff.linear_up.bias', 'encoder.layers.3.adapter_layer_ff.linear_up.weight', 'encoder.layers.4.adapter_layer_attn.layernorm.bias', 'encoder.layers.4.adapter_layer_attn.layernorm.weight', 'encoder.layers.4.adapter_layer_attn.linear_down.bias', 'encoder.layers.4.adapter_layer_attn.linear_down.weight', 'encoder.layers.4.adapter_layer_attn.linear_up.bias', 'encoder.layers.4.adapter_layer_attn.linear_up.weight', 'encoder.layers.4.adapter_layer_ff.layernorm.bias', 'encoder.layers.4.adapter_layer_ff.layernorm.weight', 'encoder.layers.4.adapter_layer_ff.linear_down.bias', 'encoder.layers.4.adapter_layer_ff.linear_down.weight', 'encoder.layers.4.adapter_layer_ff.linear_up.bias', 'encoder.layers.4.adapter_layer_ff.linear_up.weight', 'encoder.layers.5.adapter_layer_attn.layernorm.bias', 'encoder.layers.5.adapter_layer_attn.layernorm.weight', 'encoder.layers.5.adapter_layer_attn.linear_down.bias', 'encoder.layers.5.adapter_layer_attn.linear_down.weight', 'encoder.layers.5.adapter_layer_attn.linear_up.bias', 'encoder.layers.5.adapter_layer_attn.linear_up.weight', 'encoder.layers.5.adapter_layer_ff.layernorm.bias', 'encoder.layers.5.adapter_layer_ff.layernorm.weight', 'encoder.layers.5.adapter_layer_ff.linear_down.bias', 'encoder.layers.5.adapter_layer_ff.linear_down.weight', 'encoder.layers.5.adapter_layer_ff.linear_up.bias', 'encoder.layers.5.adapter_layer_ff.linear_up.weight', 'encoder.layers.6.adapter_layer_attn.layernorm.bias', 'encoder.layers.6.adapter_layer_attn.layernorm.weight', 'encoder.layers.6.adapter_layer_attn.linear_down.bias', 'encoder.layers.6.adapter_layer_attn.linear_down.weight', 'encoder.layers.6.adapter_layer_attn.linear_up.bias', 'encoder.layers.6.adapter_layer_attn.linear_up.weight', 'encoder.layers.6.adapter_layer_ff.layernorm.bias', 'encoder.layers.6.adapter_layer_ff.layernorm.weight', 'encoder.layers.6.adapter_layer_ff.linear_down.bias', 'encoder.layers.6.adapter_layer_ff.linear_down.weight', 'encoder.layers.6.adapter_layer_ff.linear_up.bias', 'encoder.layers.6.adapter_layer_ff.linear_up.weight', 'encoder.layers.7.adapter_layer_attn.layernorm.bias', 'encoder.layers.7.adapter_layer_attn.layernorm.weight', 'encoder.layers.7.adapter_layer_attn.linear_down.bias', 'encoder.layers.7.adapter_layer_attn.linear_down.weight', 'encoder.layers.7.adapter_layer_attn.linear_up.bias', 'encoder.layers.7.adapter_layer_attn.linear_up.weight', 'encoder.layers.7.adapter_layer_ff.layernorm.bias', 'encoder.layers.7.adapter_layer_ff.layernorm.weight', 'encoder.layers.7.adapter_layer_ff.linear_down.bias', 'encoder.layers.7.adapter_layer_ff.linear_down.weight', 'encoder.layers.7.adapter_layer_ff.linear_up.bias', 'encoder.layers.7.adapter_layer_ff.linear_up.weight', 'encoder.layers.8.adapter_layer_attn.layernorm.bias', 'encoder.layers.8.adapter_layer_attn.layernorm.weight', 'encoder.layers.8.adapter_layer_attn.linear_down.bias', 'encoder.layers.8.adapter_layer_attn.linear_down.weight', 'encoder.layers.8.adapter_layer_attn.linear_up.bias', 'encoder.layers.8.adapter_layer_attn.linear_up.weight', 'encoder.layers.8.adapter_layer_ff.layernorm.bias', 'encoder.layers.8.adapter_layer_ff.layernorm.weight', 'encoder.layers.8.adapter_layer_ff.linear_down.bias', 'encoder.layers.8.adapter_layer_ff.linear_down.weight', 'encoder.layers.8.adapter_layer_ff.linear_up.bias', 'encoder.layers.8.adapter_layer_ff.linear_up.weight', 'encoder.layers.9.adapter_layer_attn.layernorm.bias', 'encoder.layers.9.adapter_layer_attn.layernorm.weight', 'encoder.layers.9.adapter_layer_attn.linear_down.bias', 'encoder.layers.9.adapter_layer_attn.linear_down.weight', 'encoder.layers.9.adapter_layer_attn.linear_up.bias', 'encoder.layers.9.adapter_layer_attn.linear_up.weight', 'encoder.layers.9.adapter_layer_ff.layernorm.bias', 'encoder.layers.9.adapter_layer_ff.layernorm.weight', 'encoder.layers.9.adapter_layer_ff.linear_down.bias', 'encoder.layers.9.adapter_layer_ff.linear_down.weight', 'encoder.layers.9.adapter_layer_ff.linear_up.bias', 'encoder.layers.9.adapter_layer_ff.linear_up.weight', 'lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
2.6.0+cu124
2.6.0+cu124
frozen:  wavlm.masked_spec_embed
frozen:  wavlm.feature_extractor.conv_layers.0.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.bias
frozen:  wavlm.feature_extractor.conv_layers.1.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.2.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.3.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.4.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.5.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.6.conv.weight
frozen:  wavlm.feature_projection.layer_norm.weight
frozen:  wavlm.feature_projection.layer_norm.bias
frozen:  wavlm.feature_projection.projection.weight
frozen:  wavlm.feature_projection.projection.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0
frozen:  wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1
frozen:  wavlm.encoder.layer_norm.weight
frozen:  wavlm.encoder.layer_norm.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.0.attention.k_proj.weight
frozen:  wavlm.encoder.layers.0.attention.k_proj.bias
frozen:  wavlm.encoder.layers.0.attention.v_proj.weight
frozen:  wavlm.encoder.layers.0.attention.v_proj.bias
frozen:  wavlm.encoder.layers.0.attention.q_proj.weight
frozen:  wavlm.encoder.layers.0.attention.q_proj.bias
frozen:  wavlm.encoder.layers.0.attention.out_proj.weight
frozen:  wavlm.encoder.layers.0.attention.out_proj.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.bias
frozen:  wavlm.encoder.layers.0.attention.rel_attn_embed.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.0.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.bias
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.0.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.1.attention.k_proj.weight
frozen:  wavlm.encoder.layers.1.attention.k_proj.bias
frozen:  wavlm.encoder.layers.1.attention.v_proj.weight
frozen:  wavlm.encoder.layers.1.attention.v_proj.bias
frozen:  wavlm.encoder.layers.1.attention.q_proj.weight
frozen:  wavlm.encoder.layers.1.attention.q_proj.bias
frozen:  wavlm.encoder.layers.1.attention.out_proj.weight
frozen:  wavlm.encoder.layers.1.attention.out_proj.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.1.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.layer_norm.bias
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.1.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.2.attention.k_proj.weight
frozen:  wavlm.encoder.layers.2.attention.k_proj.bias
frozen:  wavlm.encoder.layers.2.attention.v_proj.weight
frozen:  wavlm.encoder.layers.2.attention.v_proj.bias
frozen:  wavlm.encoder.layers.2.attention.q_proj.weight
frozen:  wavlm.encoder.layers.2.attention.q_proj.bias
frozen:  wavlm.encoder.layers.2.attention.out_proj.weight
frozen:  wavlm.encoder.layers.2.attention.out_proj.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.2.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.layer_norm.bias
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.2.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.3.attention.k_proj.weight
frozen:  wavlm.encoder.layers.3.attention.k_proj.bias
frozen:  wavlm.encoder.layers.3.attention.v_proj.weight
frozen:  wavlm.encoder.layers.3.attention.v_proj.bias
frozen:  wavlm.encoder.layers.3.attention.q_proj.weight
frozen:  wavlm.encoder.layers.3.attention.q_proj.bias
frozen:  wavlm.encoder.layers.3.attention.out_proj.weight
frozen:  wavlm.encoder.layers.3.attention.out_proj.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.3.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.layer_norm.bias
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.3.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.4.attention.k_proj.weight
frozen:  wavlm.encoder.layers.4.attention.k_proj.bias
frozen:  wavlm.encoder.layers.4.attention.v_proj.weight
frozen:  wavlm.encoder.layers.4.attention.v_proj.bias
frozen:  wavlm.encoder.layers.4.attention.q_proj.weight
frozen:  wavlm.encoder.layers.4.attention.q_proj.bias
frozen:  wavlm.encoder.layers.4.attention.out_proj.weight
frozen:  wavlm.encoder.layers.4.attention.out_proj.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.4.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.layer_norm.bias
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.4.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.5.attention.k_proj.weight
frozen:  wavlm.encoder.layers.5.attention.k_proj.bias
frozen:  wavlm.encoder.layers.5.attention.v_proj.weight
frozen:  wavlm.encoder.layers.5.attention.v_proj.bias
frozen:  wavlm.encoder.layers.5.attention.q_proj.weight
frozen:  wavlm.encoder.layers.5.attention.q_proj.bias
frozen:  wavlm.encoder.layers.5.attention.out_proj.weight
frozen:  wavlm.encoder.layers.5.attention.out_proj.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.5.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.layer_norm.bias
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.5.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.6.attention.k_proj.weight
frozen:  wavlm.encoder.layers.6.attention.k_proj.bias
frozen:  wavlm.encoder.layers.6.attention.v_proj.weight
frozen:  wavlm.encoder.layers.6.attention.v_proj.bias
frozen:  wavlm.encoder.layers.6.attention.q_proj.weight
frozen:  wavlm.encoder.layers.6.attention.q_proj.bias
frozen:  wavlm.encoder.layers.6.attention.out_proj.weight
frozen:  wavlm.encoder.layers.6.attention.out_proj.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.6.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.layer_norm.bias
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.6.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.7.attention.k_proj.weight
frozen:  wavlm.encoder.layers.7.attention.k_proj.bias
frozen:  wavlm.encoder.layers.7.attention.v_proj.weight
frozen:  wavlm.encoder.layers.7.attention.v_proj.bias
frozen:  wavlm.encoder.layers.7.attention.q_proj.weight
frozen:  wavlm.encoder.layers.7.attention.q_proj.bias
frozen:  wavlm.encoder.layers.7.attention.out_proj.weight
frozen:  wavlm.encoder.layers.7.attention.out_proj.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.7.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.layer_norm.bias
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.7.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.8.attention.k_proj.weight
frozen:  wavlm.encoder.layers.8.attention.k_proj.bias
frozen:  wavlm.encoder.layers.8.attention.v_proj.weight
frozen:  wavlm.encoder.layers.8.attention.v_proj.bias
frozen:  wavlm.encoder.layers.8.attention.q_proj.weight
frozen:  wavlm.encoder.layers.8.attention.q_proj.bias
frozen:  wavlm.encoder.layers.8.attention.out_proj.weight
frozen:  wavlm.encoder.layers.8.attention.out_proj.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.8.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.layer_norm.bias
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.8.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.9.attention.k_proj.weight
frozen:  wavlm.encoder.layers.9.attention.k_proj.bias
frozen:  wavlm.encoder.layers.9.attention.v_proj.weight
frozen:  wavlm.encoder.layers.9.attention.v_proj.bias
frozen:  wavlm.encoder.layers.9.attention.q_proj.weight
frozen:  wavlm.encoder.layers.9.attention.q_proj.bias
frozen:  wavlm.encoder.layers.9.attention.out_proj.weight
frozen:  wavlm.encoder.layers.9.attention.out_proj.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.9.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.layer_norm.bias
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.9.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.10.attention.k_proj.weight
frozen:  wavlm.encoder.layers.10.attention.k_proj.bias
frozen:  wavlm.encoder.layers.10.attention.v_proj.weight
frozen:  wavlm.encoder.layers.10.attention.v_proj.bias
frozen:  wavlm.encoder.layers.10.attention.q_proj.weight
frozen:  wavlm.encoder.layers.10.attention.q_proj.bias
frozen:  wavlm.encoder.layers.10.attention.out_proj.weight
frozen:  wavlm.encoder.layers.10.attention.out_proj.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.10.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.layer_norm.bias
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.10.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.11.attention.k_proj.weight
frozen:  wavlm.encoder.layers.11.attention.k_proj.bias
frozen:  wavlm.encoder.layers.11.attention.v_proj.weight
frozen:  wavlm.encoder.layers.11.attention.v_proj.bias
frozen:  wavlm.encoder.layers.11.attention.q_proj.weight
frozen:  wavlm.encoder.layers.11.attention.q_proj.bias
frozen:  wavlm.encoder.layers.11.attention.out_proj.weight
frozen:  wavlm.encoder.layers.11.attention.out_proj.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_down.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.linear_up.bias
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.weight
enc_adapter_attn:  wavlm.encoder.layers.11.adapter_layer_attn.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.layer_norm.bias
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_down.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.linear_up.bias
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.weight
enc_adapter_ff:  wavlm.encoder.layers.11.adapter_layer_ff.layernorm.bias
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.bias
down_param:  lm_head.weight
down_param:  lm_head.bias

count of parameters:  9560096 


count of adapter_parameters:  9498624 

  0%|          | 0/100 [00:00<?, ?it/s]
