 
---------- Step 0: Installing libraries  --------------
 
---------- Step 1: Running model ----------------------
/home/skscla001/.local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Some weights of AdaWavLMForCTC were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.adapter_to_output.10.linear_down.weight', 'encoder.adapter_to_output.3.layernorm.weight', 'encoder.adapter_to_output.2.linear_down.weight', 'encoder.adapter_to_output.8.layernorm.bias', 'encoder.adapter_to_output.0.layernorm.bias', 'encoder.adapter_to_output.2.linear_down.bias', 'encoder.adapter_to_output.6.layernorm.bias', 'encoder.adapter_to_output.8.linear_down.bias', 'encoder.adapter_to_output.4.layernorm.weight', 'encoder.adapter_to_output.1.layernorm.bias', 'encoder.adapter_to_output.1.layernorm.weight', 'encoder.adapter_to_output.7.linear_down.weight', 'encoder.adapter_to_output.6.layernorm.weight', 'encoder.adapter_to_output.0.linear_down.bias', 'encoder.adapter_to_output.5.layernorm.bias', 'encoder.adapter_to_output.7.layernorm.bias', 'encoder.adapter_to_output.4.linear_down.weight', 'encoder.adapter_to_output.8.layernorm.weight', 'lm_head.bias', 'encoder.adapter_to_output.9.linear_down.bias', 'encoder.adapter_to_output.1.linear_down.weight', 'encoder.adapter_to_output.11.linear_down.weight', 'encoder.adapter_to_output_layer_weights', 'encoder.adapter_to_output.3.layernorm.bias', 'encoder.adapter_to_output.10.layernorm.bias', 'encoder.adapter_to_output.11.layernorm.weight', 'encoder.adapter_to_output.9.layernorm.bias', 'encoder.adapter_to_output.2.layernorm.bias', 'encoder.adapter_to_output.6.linear_down.bias', 'encoder.adapter_to_output.9.layernorm.weight', 'encoder.adapter_to_output.8.linear_down.weight', 'encoder.adapter_to_output.0.linear_down.weight', 'encoder.adapter_to_output.2.layernorm.weight', 'encoder.adapter_to_output.10.linear_down.bias', 'encoder.adapter_to_output.7.layernorm.weight', 'encoder.adapter_to_output.4.layernorm.bias', 'encoder.adapter_to_output.9.linear_down.weight', 'encoder.adapter_to_output.5.layernorm.weight', 'encoder.adapter_to_output.5.linear_down.weight', 'encoder.adapter_to_output.6.linear_down.weight', 'encoder.adapter_to_output.7.linear_down.bias', 'encoder.adapter_to_output.4.linear_down.bias', 'encoder.adapter_to_output.5.linear_down.bias', 'encoder.adapter_to_output.11.linear_down.bias', 'encoder.adapter_to_output.1.linear_down.bias', 'encoder.adapter_to_output.10.layernorm.weight', 'encoder.adapter_to_output.3.linear_down.weight', 'encoder.adapter_to_output.11.layernorm.bias', 'encoder.adapter_to_output.3.linear_down.bias', 'lm_head.weight', 'encoder.adapter_to_output.0.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/skscla001/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
frozen:  wavlm.masked_spec_embed
frozen:  wavlm.feature_extractor.conv_layers.0.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.weight
frozen:  wavlm.feature_extractor.conv_layers.0.layer_norm.bias
frozen:  wavlm.feature_extractor.conv_layers.1.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.2.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.3.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.4.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.5.conv.weight
frozen:  wavlm.feature_extractor.conv_layers.6.conv.weight
frozen:  wavlm.feature_projection.layer_norm.weight
frozen:  wavlm.feature_projection.layer_norm.bias
frozen:  wavlm.feature_projection.projection.weight
frozen:  wavlm.feature_projection.projection.bias
adapter_to_output_layer_weights:  wavlm.encoder.adapter_to_output_layer_weights
frozen:  wavlm.encoder.pos_conv_embed.conv.bias
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_g
frozen:  wavlm.encoder.pos_conv_embed.conv.weight_v
frozen:  wavlm.encoder.layer_norm.weight
frozen:  wavlm.encoder.layer_norm.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.0.attention.k_proj.weight
frozen:  wavlm.encoder.layers.0.attention.k_proj.bias
frozen:  wavlm.encoder.layers.0.attention.v_proj.weight
frozen:  wavlm.encoder.layers.0.attention.v_proj.bias
frozen:  wavlm.encoder.layers.0.attention.q_proj.weight
frozen:  wavlm.encoder.layers.0.attention.q_proj.bias
frozen:  wavlm.encoder.layers.0.attention.out_proj.weight
frozen:  wavlm.encoder.layers.0.attention.out_proj.bias
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.0.attention.gru_rel_pos_linear.bias
frozen:  wavlm.encoder.layers.0.attention.rel_attn_embed.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.layer_norm.bias
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.0.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.0.final_layer_norm.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.1.attention.k_proj.weight
frozen:  wavlm.encoder.layers.1.attention.k_proj.bias
frozen:  wavlm.encoder.layers.1.attention.v_proj.weight
frozen:  wavlm.encoder.layers.1.attention.v_proj.bias
frozen:  wavlm.encoder.layers.1.attention.q_proj.weight
frozen:  wavlm.encoder.layers.1.attention.q_proj.bias
frozen:  wavlm.encoder.layers.1.attention.out_proj.weight
frozen:  wavlm.encoder.layers.1.attention.out_proj.bias
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.1.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.1.layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.layer_norm.bias
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.1.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.1.final_layer_norm.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.2.attention.k_proj.weight
frozen:  wavlm.encoder.layers.2.attention.k_proj.bias
frozen:  wavlm.encoder.layers.2.attention.v_proj.weight
frozen:  wavlm.encoder.layers.2.attention.v_proj.bias
frozen:  wavlm.encoder.layers.2.attention.q_proj.weight
frozen:  wavlm.encoder.layers.2.attention.q_proj.bias
frozen:  wavlm.encoder.layers.2.attention.out_proj.weight
frozen:  wavlm.encoder.layers.2.attention.out_proj.bias
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.2.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.2.layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.layer_norm.bias
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.2.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.2.final_layer_norm.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.3.attention.k_proj.weight
frozen:  wavlm.encoder.layers.3.attention.k_proj.bias
frozen:  wavlm.encoder.layers.3.attention.v_proj.weight
frozen:  wavlm.encoder.layers.3.attention.v_proj.bias
frozen:  wavlm.encoder.layers.3.attention.q_proj.weight
frozen:  wavlm.encoder.layers.3.attention.q_proj.bias
frozen:  wavlm.encoder.layers.3.attention.out_proj.weight
frozen:  wavlm.encoder.layers.3.attention.out_proj.bias
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.3.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.3.layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.layer_norm.bias
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.3.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.3.final_layer_norm.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.4.attention.k_proj.weight
frozen:  wavlm.encoder.layers.4.attention.k_proj.bias
frozen:  wavlm.encoder.layers.4.attention.v_proj.weight
frozen:  wavlm.encoder.layers.4.attention.v_proj.bias
frozen:  wavlm.encoder.layers.4.attention.q_proj.weight
frozen:  wavlm.encoder.layers.4.attention.q_proj.bias
frozen:  wavlm.encoder.layers.4.attention.out_proj.weight
frozen:  wavlm.encoder.layers.4.attention.out_proj.bias
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.4.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.4.layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.layer_norm.bias
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.4.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.4.final_layer_norm.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.5.attention.k_proj.weight
frozen:  wavlm.encoder.layers.5.attention.k_proj.bias
frozen:  wavlm.encoder.layers.5.attention.v_proj.weight
frozen:  wavlm.encoder.layers.5.attention.v_proj.bias
frozen:  wavlm.encoder.layers.5.attention.q_proj.weight
frozen:  wavlm.encoder.layers.5.attention.q_proj.bias
frozen:  wavlm.encoder.layers.5.attention.out_proj.weight
frozen:  wavlm.encoder.layers.5.attention.out_proj.bias
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.5.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.5.layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.layer_norm.bias
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.5.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.5.final_layer_norm.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.6.attention.k_proj.weight
frozen:  wavlm.encoder.layers.6.attention.k_proj.bias
frozen:  wavlm.encoder.layers.6.attention.v_proj.weight
frozen:  wavlm.encoder.layers.6.attention.v_proj.bias
frozen:  wavlm.encoder.layers.6.attention.q_proj.weight
frozen:  wavlm.encoder.layers.6.attention.q_proj.bias
frozen:  wavlm.encoder.layers.6.attention.out_proj.weight
frozen:  wavlm.encoder.layers.6.attention.out_proj.bias
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.6.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.6.layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.layer_norm.bias
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.6.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.6.final_layer_norm.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.7.attention.k_proj.weight
frozen:  wavlm.encoder.layers.7.attention.k_proj.bias
frozen:  wavlm.encoder.layers.7.attention.v_proj.weight
frozen:  wavlm.encoder.layers.7.attention.v_proj.bias
frozen:  wavlm.encoder.layers.7.attention.q_proj.weight
frozen:  wavlm.encoder.layers.7.attention.q_proj.bias
frozen:  wavlm.encoder.layers.7.attention.out_proj.weight
frozen:  wavlm.encoder.layers.7.attention.out_proj.bias
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.7.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.7.layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.layer_norm.bias
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.7.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.7.final_layer_norm.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.8.attention.k_proj.weight
frozen:  wavlm.encoder.layers.8.attention.k_proj.bias
frozen:  wavlm.encoder.layers.8.attention.v_proj.weight
frozen:  wavlm.encoder.layers.8.attention.v_proj.bias
frozen:  wavlm.encoder.layers.8.attention.q_proj.weight
frozen:  wavlm.encoder.layers.8.attention.q_proj.bias
frozen:  wavlm.encoder.layers.8.attention.out_proj.weight
frozen:  wavlm.encoder.layers.8.attention.out_proj.bias
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.8.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.8.layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.layer_norm.bias
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.8.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.8.final_layer_norm.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.9.attention.k_proj.weight
frozen:  wavlm.encoder.layers.9.attention.k_proj.bias
frozen:  wavlm.encoder.layers.9.attention.v_proj.weight
frozen:  wavlm.encoder.layers.9.attention.v_proj.bias
frozen:  wavlm.encoder.layers.9.attention.q_proj.weight
frozen:  wavlm.encoder.layers.9.attention.q_proj.bias
frozen:  wavlm.encoder.layers.9.attention.out_proj.weight
frozen:  wavlm.encoder.layers.9.attention.out_proj.bias
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.9.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.9.layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.layer_norm.bias
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.9.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.9.final_layer_norm.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.10.attention.k_proj.weight
frozen:  wavlm.encoder.layers.10.attention.k_proj.bias
frozen:  wavlm.encoder.layers.10.attention.v_proj.weight
frozen:  wavlm.encoder.layers.10.attention.v_proj.bias
frozen:  wavlm.encoder.layers.10.attention.q_proj.weight
frozen:  wavlm.encoder.layers.10.attention.q_proj.bias
frozen:  wavlm.encoder.layers.10.attention.out_proj.weight
frozen:  wavlm.encoder.layers.10.attention.out_proj.bias
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.10.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.10.layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.layer_norm.bias
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.10.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.10.final_layer_norm.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_const
frozen:  wavlm.encoder.layers.11.attention.k_proj.weight
frozen:  wavlm.encoder.layers.11.attention.k_proj.bias
frozen:  wavlm.encoder.layers.11.attention.v_proj.weight
frozen:  wavlm.encoder.layers.11.attention.v_proj.bias
frozen:  wavlm.encoder.layers.11.attention.q_proj.weight
frozen:  wavlm.encoder.layers.11.attention.q_proj.bias
frozen:  wavlm.encoder.layers.11.attention.out_proj.weight
frozen:  wavlm.encoder.layers.11.attention.out_proj.bias
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.weight
frozen:  wavlm.encoder.layers.11.attention.gru_rel_pos_linear.bias
layer_norm:  wavlm.encoder.layers.11.layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.layer_norm.bias
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.intermediate_dense.bias
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.weight
frozen:  wavlm.encoder.layers.11.feed_forward.output_dense.bias
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.weight
layer_norm:  wavlm.encoder.layers.11.final_layer_norm.bias
adapter_output:  wavlm.encoder.adapter_to_output.0.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.0.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.0.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.0.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.1.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.1.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.1.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.1.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.2.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.2.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.2.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.2.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.3.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.3.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.3.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.3.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.4.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.4.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.4.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.4.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.5.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.5.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.5.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.5.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.6.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.6.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.6.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.6.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.7.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.7.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.7.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.7.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.8.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.8.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.8.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.8.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.9.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.9.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.9.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.9.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.10.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.10.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.10.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.10.layernorm.bias
adapter_output:  wavlm.encoder.adapter_to_output.11.linear_down.weight
adapter_output:  wavlm.encoder.adapter_to_output.11.linear_down.bias
adapter_output:  wavlm.encoder.adapter_to_output.11.layernorm.weight
adapter_output:  wavlm.encoder.adapter_to_output.11.layernorm.bias
down_param:  lm_head.weight
down_param:  lm_head.bias

count of parameters:  4790316 


count of adapter_parameters:  4737036 

  0%|          | 0/100 [00:00<?, ?it/s]
Epoch 1/100 | train |  Loss: 1.2984 WER: 0.6998
Epoch 2/100 | train |  Loss: 0.4641 WER: 0.4584
Epoch 3/100 | train |  Loss: 0.3625 WER: 0.3828
Epoch 4/100 | train |  Loss: 0.3091 WER: 0.3382
Epoch 5/100 | train |  Loss: 0.2737 WER: 0.3097
Epoch 6/100 | train |  Loss: 0.2469 WER: 0.2845
Epoch 7/100 | train |  Loss: 0.2264 WER: 0.2642
Epoch 8/100 | train |  Loss: 0.2119 WER: 0.2529
Epoch 9/100 | train |  Loss: 0.1979 WER: 0.2385
Epoch 10/100 | train |  Loss: 0.1906 WER: 0.2273
Epoch 11/100 | train |  Loss: 0.1842 WER: 0.2197
Epoch 12/100 | train |  Loss: 0.1754 WER: 0.2100
Epoch 13/100 | train |  Loss: 0.1668 WER: 0.2013
Epoch 14/100 | train |  Loss: 0.1571 WER: 0.1897
Epoch 15/100 | train |  Loss: 0.1483 WER: 0.1780
Epoch 16/100 | train |  Loss: 0.1340 WER: 0.1621
Epoch 17/100 | train |  Loss: 0.1256 WER: 0.1532
Epoch 18/100 | train |  Loss: 0.1130 WER: 0.1397
Epoch 19/100 | train |  Loss: 0.1034 WER: 0.1271
Epoch 20/100 | train |  Loss: 0.0995 WER: 0.1231
Epoch 21/100 | train |  Loss: 0.0886 WER: 0.1109
Epoch 22/100 | train |  Loss: 0.0799 WER: 0.1012
Epoch 23/100 | train |  Loss: 0.0747 WER: 0.0955
Epoch 24/100 | train |  Loss: 0.0696 WER: 0.0892
Epoch 25/100 | train |  Loss: 0.0626 WER: 0.0798
Epoch 26/100 | train |  Loss: 0.0567 WER: 0.0729
Epoch 27/100 | train |  Loss: 0.0482 WER: 0.0630
Epoch 28/100 | train |  Loss: 0.0463 WER: 0.0597
Epoch 29/100 | train |  Loss: 0.0451 WER: 0.0569
Epoch 30/100 | train |  Loss: 0.0406 WER: 0.0504
Epoch 31/100 | train |  Loss: 0.0398 WER: 0.0477
Epoch 32/100 | train |  Loss: 0.0388 WER: 0.0445
Epoch 33/100 | train |  Loss: 0.0354 WER: 0.0426
Epoch 34/100 | train |  Loss: 0.0326 WER: 0.0392
Epoch 35/100 | train |  Loss: 0.0332 WER: 0.0373
Epoch 36/100 | train |  Loss: 0.0317 WER: 0.0362
Epoch 37/100 | train |  Loss: 0.0316 WER: 0.0351
Epoch 38/100 | train |  Loss: 0.0303 WER: 0.0326
Epoch 39/100 | train |  Loss: 0.0280 WER: 0.0306
Epoch 40/100 | train |  Loss: 0.0266 WER: 0.0294
Epoch 41/100 | train |  Loss: 0.0262 WER: 0.0284
Epoch 42/100 | train |  Loss: 0.0259 WER: 0.0271
Epoch 43/100 | train |  Loss: 0.0254 WER: 0.0272
Epoch 44/100 | train |  Loss: 0.0265 WER: 0.0274
Epoch 45/100 | train |  Loss: 0.0244 WER: 0.0255
Epoch 46/100 | train |  Loss: 0.0236 WER: 0.0244
Epoch 47/100 | train |  Loss: 0.0222 WER: 0.0233
Epoch 48/100 | train |  Loss: 0.0222 WER: 0.0234
Epoch 49/100 | train |  Loss: 0.0220 WER: 0.0224
Epoch 50/100 | train |  Loss: 0.0217 WER: 0.0218
Epoch 51/100 | train |  Loss: 0.0224 WER: 0.0227
Epoch 52/100 | train |  Loss: 0.0212 WER: 0.0211
Epoch 53/100 | train |  Loss: 0.0208 WER: 0.0209
Epoch 54/100 | train |  Loss: 0.0211 WER: 0.0205
Epoch 55/100 | train |  Loss: 0.0203 WER: 0.0205
Epoch 56/100 | train |  Loss: 0.0205 WER: 0.0205
Epoch 57/100 | train |  Loss: 0.0201 WER: 0.0194
Epoch 58/100 | train |  Loss: 0.0205 WER: 0.0199
Epoch 59/100 | train |  Loss: 0.0189 WER: 0.0185
Epoch 60/100 | train |  Loss: 0.0200 WER: 0.0189
Epoch 61/100 | train |  Loss: 0.0191 WER: 0.0183
Epoch 62/100 | train |  Loss: 0.0199 WER: 0.0194
Epoch 63/100 | train |  Loss: 0.0189 WER: 0.0185
Epoch 64/100 | train |  Loss: 0.0202 WER: 0.0186
Epoch 65/100 | train |  Loss: 0.0186 WER: 0.0172
Epoch 66/100 | train |  Loss: 0.0185 WER: 0.0178
Epoch 67/100 | train |  Loss: 0.0189 WER: 0.0176
Epoch 68/100 | train |  Loss: 0.0187 WER: 0.0170
Epoch 69/100 | train |  Loss: 0.0182 WER: 0.0173
Epoch 70/100 | train |  Loss: 0.0184 WER: 0.0176
Epoch 71/100 | train |  Loss: 0.0174 WER: 0.0169
Epoch 72/100 | train |  Loss: 0.0168 WER: 0.0168
Epoch 73/100 | train |  Loss: 0.0183 WER: 0.0165
Epoch 74/100 | train |  Loss: 0.0177 WER: 0.0167
Epoch 75/100 | train |  Loss: 0.0175 WER: 0.0163
Epoch 76/100 | train |  Loss: 0.0182 WER: 0.0167
Epoch 77/100 | train |  Loss: 0.0185 WER: 0.0166
Epoch 78/100 | train |  Loss: 0.0180 WER: 0.0166
Epoch 79/100 | train |  Loss: 0.0170 WER: 0.0157
Epoch 80/100 | train |  Loss: 0.0169 WER: 0.0157
Epoch 81/100 | train |  Loss: 0.0176 WER: 0.0164
Epoch 82/100 | train |  Loss: 0.0166 WER: 0.0158
Epoch 83/100 | train |  Loss: 0.0176 WER: 0.0158
Epoch 84/100 | train |  Loss: 0.0168 WER: 0.0155
Epoch 85/100 | train |  Loss: 0.0163 WER: 0.0153
